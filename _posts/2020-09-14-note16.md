---
title: 강화학습 8
categories: [study]
comments: true
---

## Temporal Difference (TD)

TD 는 MC (Monte-Carlo) 와 유사한 방법론이지만, state value function 을 업데이트하는 시점이 다르다.

MC 는 하나의 Episode 가 전부 완결되었을 때, 최종적으로 얻은 Episode 상에서의 Reward 정보를 토대로 state value function 들을 업데이트한다.

하지만 TD 는 Episode 가 진행되는 동안, time step 이 진행됨에 따라 계속해서 state value function 을 업데이트한다.

$$ V(S_t) \leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) $$

## TD 의 의의

Episode 가 전부 진행되지 않고도 state value function 을 업데이트할 수 있다는 점에 의의가 있다.

Environment 에 따라서 Episode 가 종결되기까지 오래 걸릴 수 있는것이다. 이러한 경우에 쓰일 수 있다.

## TD 가 갖는 문제점

Bias 와 Variance

## n-step TD

n 을 어떻게 결정하는가? 낮아도, 높아도 각각의 장단점이 있다.
그래서 lammda return 방법론이 나온다.

그런데 문제는 n 이 커지면 episode 의 끝을 보지 않고도 value function update 가 가능하다는 TD 의 장점이 죽어버린다.

이를 해소하는 방법이 backward TD

## lammad return

## backward TD

https://dnddnjs.gitbooks.io/rl/content/eligibility_traces.html