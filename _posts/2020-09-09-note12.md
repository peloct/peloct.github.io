---
title: 강화학습 4 (Q-Table 방법론)
categories: [study]
comments: true
---

## Q-Table 방법론

Q-Table 방법론은 Environment 를 제한된 상태로 표현할 수 있고 액션 또한 한정되어 있는 경우, action-value function 를 학습시키는 방법론이다.

Policy 는 action-value function 을 기반으로, 현재 상태에서 argmax 한 행동을 선택하는 것으로 한다. (Exploiting 을 위해 때때로 다른 행동을 하도록 랜덤값을 넣기도 한다.)

action-value function 은 "어떤 상태에, 어떤 행동을 하면 최종적으로 몇 점을 기대할 수 있는가?" 를 말하기 때문에, 이를 별도의 테이블(Q-Table)에 저장한다고 하면 **상태 수 x 액션 수** 사이즈의 테이블이 요구된다.

아이디어는 여러 번의 꽤 많은 episode 를 거치면서 다음의 식으로 action-value function 을 개선하는 것이다.

$$ Q[ curState, action ] = R + \gamma * \max_{a \in Action} Q[ nextState, a] $$

$$ R $$ 는 방금 action 을 취함으로써 Environment 로부터 얻은 reward 이다.

문제는 수식은 Environment 가 deterministic 한 경우에만 통한다. Environment 에 따라서 같은 상태에서 이전과 같은 action 을 취했을 때, 이전의 경험과는 다른 상태에 도달하는 Environment 또한 있다.

이러한 경우에 위의 수식은 다음과 같이 바뀐다.

$$ Q[ curState, action ] = (1 - \alpha) * Q[ curState, action ] $$

$$ + \alpha * (R + \gamma * \max_{a \in Action} Q[ nextState, a]) $$

요컨대 지금의 실패나 성공을 반드시 믿으며 이전의 경험을 날리지 않고, 이전의 경험을 조금씩 남기는 것이다. ($$ \alpha $$ 는 learning rate 라 하며 0부터 1사이의 값이다.)

~~~ python
import gym
import numpy as np

env = gym.make('FrozenLake-v0')
env.render()

Q = np.zeros([env.observation_space.n, env.action_space.n])
num_episodes = 5000
discount_factor = 0.99
learning_rate = 0.85

rList = []
for i in range(num_episodes):
    state = env.reset()
    done = False
    e = 1 / ((i // 100) + 1)

    while not done:
        action = np.argmax(Q[state, :] + np.random.randn(1, env.action_space.n) / (i + 1))
        new_state, reward, done, _ = env.step(action)
        Q[state, action] =\
            (1 - learning_rate) * Q[state, action]\
            + learning_rate * (reward + discount_factor * np.max(Q[new_state, :]))
        state = new_state
    rList.append(rAll)

print("Success rate: " + str(sum(rList) / num_episodes))
print(Q)
~~~

위는 Stochastic 한 Environment 인 FrozenLake 를 푸는 Q-Table Method 의 구현이다.