---
title: 머신러닝
categories: [study]
comments: true
---

## Type of supervised learning

score => regression

1 아니면 0 => binary classification

1, 2, 3, 4, 5 => multilevel classification

시간에 따른 시험점수같은 것을 학습 시킨다면 : regression model

시간에 따라 합격, 불합격을 학습 시킨다면 : binary classification

시간에 따라 A B D F : multilevel classification

## Tensorflow 에서 Tensor 정의

rank : 몇 차원 array 인가?

1차원이면 벡터, 2차원이면 행렬, 3차원이면 3차원 텐서...

shape : 차원 별로의 size

(2 x 2 행렬 인가? 3 x 5 x 2 x 7 tensor 인가? 등)

type : 데이터 타입

## Linear regression

Linear regression model 은 linear 한 모델이 데이터에 맞을 것이다 라고 가정한다.

목적은 데이터들과의 오차의 제곱 평균을 최소화하는 직선 구하기. (기울기 및 y절편)

가정함수(예측함수)는 다음과 같다.

$$ H(x) = Wx + b $$

여기서 **cost(W, b) = 오차의 제곱 평균** 이 가장 낮아지게 하는 W, b 를 구하는 것이 목적이다.

Tensorflow 는 수식을 graph 의 형태로 만들기에, Backpropagation 을 통해 원인에 의한 최종 함수의 변화(미분) 을 계산할 수 있다.

전체 함수의 미분을 알면, Descent Gradient 도 가능하다.

따라서 cost 함수를 Tensorflow 의 그래프로 표현하고 Descent Gradient 를 통해 optimize 하면 W, b 를 구하는 것이 가능하다.

## Neural Network

Neural Network 도 결국 input 들과 weight 들에 대한 최종 output 의 함수이며, 따라서 Optimize 가 가능하다.

Activation function 이란 neural 의 활성화, 비활성화 정도를 나타내는 함수로, Linear Combination 된 벡터에 Component wise 하게 적용된 이후, 다음 Layer 의 입력으로 들어가게 된다.

Backpropagation 을 이용하는 것은 좋은데, Network 가 깊어지면 깊어질수록 출력으로부터 멀리 있는 변수의 영향이 작아진다.(Vasnishing Gradient) 이때 각 레이어의 Activation function 으로 Sigmoid 를 사용하지 않고 ReLU 를 사용하면 좋다.(왜?)
(ReLU 외에도 여러 Activation function 들이 존재한다.)

Overfitting : 데이터를 학습시키다보면 학습 데이터 내에서의 정확도는 오르지만, 점차 학습하지 않은 다른 데이터에 대해서는 정확도가 낮아지게 되는 현상. => Regulation 이라는 과정을 통해 완화할 수 있다.

Regulation : weignt 에 큰 값이 들어가지 않게 한다.
Dropout : 학습시킬때 랜덤하게 Layer 와 Layer 사이에 연결을 일부 끊어서 학습시킨다. (제한된 정보만으로 목적을 이루도록 학습시키는 것과 유사) (Overfitting 을 완화하는 방법론 중 하나)
Ensemble : 여러 트레이닝 셋으로 각각 학습된 모델을 이용한다. 각 모델에서 내뱉은 값을 조합해서 최종결론을 내린다.

## NN 이리저리 만들기

Fast forward : 어떤 레이어의 결과를 바로 다음 레이어에 전달하는 것 외에도, 그 이후 어떤 시점에 집어넣는다. (합연산)

Split & Merge : 전체 NN을 중간에 두갈래로 나눴다가 합친다. 처음 입력단부터 이미 여러 NN 갈래로 시작한다.

Recurrent Network : 뭔가 노드를 그리드 형태로 만든 다음, 아래에서 위로 뿐만 아니라 왼쪽에서 오른쪽으로.

어쨌든 별에 별 이상한 NN 형태 만들기가 있다. 해봤더니 잘되더라? 하는 느낌.

## CNN (Convolutional Neural Networks)

기본은 입력을 바로 사용하는 것이 아니라, Filter 라는 것을 이용해서 입력의 부분부분에 filter 를 적용, filter 의 결과를 이용하는 것이다. 이 과정을 Feature extraction 이라고 부른다.

Convolutional Layer (CL)

filter 를 통해서 convolution 하여 그 결과를 내보내는 레이어다.

filter 적용 규칙의 명시

shape : 필터의 사이즈
stride : 몇 칸씩 움직이며 적용할 것인가?
padding : 입력에 줄 padding (convolution 결과의 shape 이 원본 입력의 shape 과 같게 하는 등의 목적을 위해)
필터의 개수 : 하나의 입력으로부터 몇 가지의 서로 다른 output 을 낸다.

Pooling Layer (PL)

Pooling 은 입력을 읽고 모종의 규칙으로 샘플링한 결과를 내보내는 레이어다. (그런데 이게 Convolution 아닌가?)

CNN 은 대강 다음과 같은 구조이다.

입력 - CL1 - PL1 - CL2 - PL2 - ... - PLn - 일반적인 DNN

## RNN (Recurrent Neural Network)

순차적으로 발생하는 사건 등을 학습시키기에 좋다.

