---
title: 강화학습 2
categories: [study]
comments: true
---

Markov Decision Process 문제를 해결한다고 하자.(즉 environment 가 fully observable, state 가 다음 state 를 결정하기에 충분한 정보를 갖는다.) 강화학습을 하기 위해선, Environment 를 Markov Decision Process 로 모델링하는 것이 중요.

## max 와 argmax

**max 함수**는 어떤 집합 내에서 가장 큰 값을 찾는 함수이다.

**argmax 함수**는 인자에 대해서 집합 내에 한 원소가 결정되고, 집합 내에서 가장 큰 원소를 찾았을 때, 그 원소에 대응되는 인자를 반환하는 함수이다. (요컨대 max 가 되게 하는 argument 를 찾는 함수)

## State Transition Matrix 의 정의

총 n 개의 state $$ s_1, s_2, \cdots , s_n $$ 가 있다고 했을 때,

$$ P_{ij} = P[ S_{t+1} = s_j \vert S_t = s_i ] $$

이것은 Random 하게 상태가 변경되는 경우에 각 state 에서 다른 state 로 이동할 확률을 명시한 행렬이다.

## Markov Process

Markov Process 는 다음과 같이 명시할 수 있다.

$$ < S, P > $$

$$ S $$ : 모든 상태 집합

$$ P $$ : State Transition Matrix

이 위에서 랜덤하게 State 와 State 를 오간다.

## Episode

Episode 의 정의는 다음과 같다.

**어느 state 에서 시작해서 마지막 state 까지 가는 과정**

샘플링이라는 것은 확률변수로부터 이벤트가 발생한다는 것을 말하는데, 어떤 확률분포를 갖는 확률변수의 한 사건을 추출한다는 것. 이러한 측면에서 Markov Process 로부터 샘플링을 수행한 것을 episode 라고 볼 수 있겠다.

## Markov Reward Process

Markov Reward Process 는 다음과 같이 명시할 수 있다.

$$ < S, P, R, \gamma > $$

일반적인 Markov Process 와는 달리 각 상태에 점수가 부여되어 있으며, 해당 상태에 진입할 때마다 

$$ S $$ : 모든 상태 집합

$$ P $$ : State Transition Matrix

$$ R $$ : reward function

$$ \gamma $$ : discount factor

이때 reward function 이란 다음과 같이 정의된다.

$$ R_s \equiv E[ Re_{t+1} | S_t = s] $$

어떤 상태에 있을 때, 다음으로 얻을 Reward($$ Re $$) 의 기대값을 말한다.

## Return

강화학습은 return $$ G_t $$ 을 maximize 하는 프로세스이다.

이때 return 은 다음과 같이 정의된다.

$$ G_t \equiv Re_{t+1} + \gamma Re_{t + 2} + \gamma^2 Re_{t + 3} + \cdots $$

각 보상 Reward($$ Re $$) 별로 앞에 $$ \gamma $$ 가 붙어서 미래의 보상에는 좀 더 낮은 가중치를 준다.

discount factor ($$ \gamma $$) 를 넣는 이유는 수학적으로 편리하기 때문이라고 한다. 수렴성이 증명된다나.

Markov reward process 가 최종적으로 terminate 될 수 있다면 gamma 가 1 이어도 될 수도 있다.

## MRP 의 Value function

MRP 에서의 value function 은 return 의 기대값.

episode 를 샘플링하면, 거기에서 return 이 결정되니까 이 return 도 확률변수이다.

$$ v(s) = E[ G_t \vert S_t = s ] $$

## Bellman Equation for MRP

벨만 방정식이라고 부른다.

위의 정의에 따라서 어떤 상태 $$ s $$ 에서의 value function 과 다음 상태들 $$ s' $$ 의 value function 사이에 무슨 관계가 있는지를 알려준다.

$$ v(s) = E[ G_t \vert S_t = s ] $$

$$ = E[ Re_{t+1} + \gamma Re_{t + 2} + \cdots \vert S_t = s ] $$

$$ = E[ Re_{t+1} + \gamma G_{t+1} \vert S_t = s ] $$

$$ = E[ Re_{t+1} + \gamma v(S_{t + 1}) \vert S_t = s ] $$

$$ = R_s + \gamma \sum_{s' \in S}P_{ss'}v(s') $$

이때,

$$ V^T = (v(1), v(2), ... , v(n)) $$

$$ R^T = (R_1, R_2, ... , R_n) $$

이라 하면,

$$ V = R + \gamma PV $$

$$ (I - \gamma P) V = R $$

$$ V = (I - \gamma P)^{-1} R $$

즉, MRP 에서는 value function 이 바로 계산되어진다.

(하지만 계산복잡도가 O^3 이기 때문에, 큰 MRP 문제에서는 Dynamic programming, Monte-Carlo evaluation, Temporal-Difference Learning 을 사용한다.)

## Markov Decision Process

Markov Reward Process 는 다음과 같이 명시할 수 있다.

$$ < S, A, P, R, \gamma > $$

$$ S $$ : 모든 상태 집합

$$ A $$ : action 의 집합

$$ P $$ : State Transition Matrix

$$ R $$ : reward function

$$ \gamma $$ : discount factor

이때 $$ P $$ 는 MP, MRP 와는 살짝 변형된다.

$$ P_{ss'}^a \equiv $$ action a 를 했을 때, state 가 s 에서 s' 로 바뀔 확률이다. 임의의 상태에서 어떤 액션을 했을 때, 이동한 상태가 항상 일정하다면 이를 **Deterministic**, 확률적으로 다른 상태로 이동하기도 한다면 **Stochastic** 하다라고 한다.

또한 $$ R $$ 도 살짝 변형된다.

$$ R_s^a \equiv $$ state s 에서 action a 를 수행했을 때 얻는 $$ Re_{t + 1} $$ 의 기댓값

이번에는 Action 이라는 개념이 추가되어서, Policy 에 따라 다른 action 을 취할 수 있는 가능성을 넣었다. MRP 에서는 랜덤프로세스였지만, MDP 에서는 어떤 액션을 수행할지 선택하는 것이다. 그래서 어떤 policy 를 갖고 액션을 수행할지가 중요하다.

만약 Action 을 하면 확률적으로 다른 state 로 간다.

우선 Agent 가 사용하는 policy 를 수학적으로 다음과 같이 모델링한다.

$$ \pi(a\vert s) = P[ A_t = a \vert S_t = s ] $$

MDP 에서 어떤 policy 를 갖고 움직일 때, 만약 policy 가 고정이라면, policy 또한 위의 함수처럼 확률적으로 Action을 결정하기에, MDP 를 MRP 로 볼 수 있다.

이때 MRP 의 명시는 다음과 같이 된다.

$$ < S, P^\pi, R^\pi, \gamma > $$

$$ P^\pi $$ 는 다음과 같이 정의된다.

$$ P_{s,s'}^\pi \equiv \sum_{a \in A} \pi(a \vert s)P_{ss'}^a $$

$$ R^\pi $$ 는 다음과 같이 정의된다.

$$ R_s^\pi \equiv \sum_{a \in A} \pi(a \vert s)R_s^a $$

state 의 변화만을 본다면, $$ < S, P^\pi > $$ 로 명시된 MP 가 된다.

## MDP 의 Value function

state-value function $$ v_\pi(s) $$ 는 policy $$ \pi $$ 를 따라서 끝까지 했을 때, return 의 기대값으로 정의된다.

action-value function $$ q_\pi(s, a) $$ 는 현재 state s 에서 행동 a 를 했을 때, policy $$ \pi $$ 를 따랐을 경우의 return 의 기대값. 이것의 좋은 점은 현재 상태가 있을때, 어떤 행동을 하면 좋냐? 라는 질문에 대한 실질적인 답변을 해준다는 점이다.

### Bellman Expextation Equation for MDP

$$ v_\pi(s) = E_\pi[ Re_{t+1} + \gamma v_\pi(S_{t + 1}) \vert S_t = s ] $$

$$ q_\pi(s, a) = E_\pi[ Re_{t+1} + \gamma q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a ] $$

v 와 q 사이의 관계는 다음과 같다.

$$ v_\pi(s) = \sum_{a \in A}\pi(a \vert s)q_\pi(s, a) $$

$$ q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s') $$

따라서

$$ v_\pi(s) = \sum_{a \in A}\pi(a \vert s)(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s')) $$

$$ q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a ( v_\pi(s') = \sum_{a' \in A}\pi(a' \vert s')q_\pi(s', a') ) $$

Bellman Expextation Equation 의 Matrix Form 은 다음과 같다.

$$ v_\pi = R^\pi + \gamma P^\pi v_\pi $$

$$ v_\pi = (I - \gamma P^\pi)^{-1} R^\pi $$

## Optimal value function

optimal state-value function $$ v_{*}(s) $$ 는 다음을 말한다.

$$ v_{*}(s) = \max_\pi v_\pi(s) $$

$$ q_{*}(s, a) = \max_\pi q_\pi(s, a) $$

optimal value function 을 알 때, **MDP가 풀렸다.** 라고 한다.

## Optimal policy

이 경우 policy 와 policy 는 partial ordering 을 갖는다. (집합론으로 치면, ordering 이 가능한 페어와 불가능한 페어가 있다는 것)

$$ \pi \geq \pi' \quad if \quad v_\pi(s) \geq v_{\pi'}(s), \forall s $$

## MDP 에 대한 Theorem

- 모든 다른 policy 에 대해서 좋거나 같은 optimal policy 가 존재한다.
- $$ v_{\pi*}(s) = v_{*}(s) $$
- $$ q_{\pi*}(s, a) = q_{*}(s, a) $$
- optimal policy 는 $$ q_{*}(s, a) $$ 로부터 구해질 수 있다.
$$ \pi_{ *}(a \vert s) = 1 \quad if \quad a = \max_{a \in A} q_{ *}(s, a) \quad else \quad 0 $$
- 어떤 MDP 에도 언제나 deterministic optimal policy 가 있다.
- $$ q_{*}(s, a) $$ 를 알면, 바로 optimal policy 를 얻는다.

### Bellman Optimality Equation for MDP

$$ v_{*}(s) = \max_{a \in A}q_{ *}(s, a) $$

$$ q_{*}(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{ *}(s') $$

$$ v_{*}(s) = \max_{a \in A}( R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{ *}(s')) $$

max 로 인해서 linear equation 이 아니기에, 바로 풀 수 없다. (no closed form solution)

이를 풀기 위한 방법론들은 다음과 같다.

- Value Iteration
- Policy Iteration
- Q-learning
- Sarsa