---
title: 강화학습 5 (Q-Network 방법론)
categories: [study]
comments: true
---

Q-Table 방법론은 Environment 의 상태와 액션의 수가 적은 경우 사용할 수 있는 방법론이며, Table 이 커짐에 따라, action-value function 을 근사할 수 있는 또 다른 방법론이 필요해진다.

## Q-Network

Q-Network 는 결국 상태를 입력으로 받으면 액션들의 점수(현재 상황에서 어떤 액션은 몇 점을 기대가능 등)를 반환하는 함수이다. 그리고 그것은 Neural Network 를 통해서 구현된다.

현재 나의 이해로는 세상 어딘가에 optimal 한 action-value function 이 있다고 믿고, Q-Network 는 이 action-value function 을 비교적 적은 인자를 통해서 근사하는 것을 목적으로 하는 방법론이다.

Neural Netwrok 은 결국 Weight 의 모임이고, 각 Weight, 입력에 대한 최종 값의 미분을 구할 수 있기 때문에 Gradient descent method 를 통해서 Optimizing 하는 것이 가능하다.


~~~
init Q with random weights

for episode from 1 to M do
    setup environment

    for t from 1 to T do
        p = Q(state)
        action = argmax(p)
        nextState, reward = environment.step(action)
        
        if isTerminal(nextState)
            y = reward
        else
            y = reward + gamma * max(Q(nextState))

        perform gradient descent step on (y - p[action])^2
~~~

모든 상태에 대해서 각 상태의 Q 가 다음 상태의 최대 Q 와 같을 때, 이 Q 가 optimal 이라는 말일까? 잘 모르겠다. 어쨌든 가장 원하는 상태는 각 Q를 통해서 가장 좋다고 판단되는 선택을 했을 때의 Q 값이, 그 선택으로 인해 이동한 곳에서의 최대 Q 와 같은 경우인듯 하다.

한 번 구현해본 코드.(잘 안된다. 잘못 구현했을 지도 모른다. Keras... 정신나갈거같아정신나갈거같아)

~~~ python
import gym
import numpy as np
import tensorflow as tf
import tensorflow.keras as keras
import tensorflow.keras.layers as layers


def one_hot(x):
    return one_hot_table[x:x + 1]


def loss_fn(y, q_prediction):
    return tf.reduce_sum(tf.square(y - q_prediction))


env = gym.make('FrozenLake-v0')
env.render()
input_size = env.observation_space.n
output_size = env.action_space.n
one_hot_table = np.identity(16)

# model define
model = keras.Sequential()
model.add(layers.Dense(output_size,
                       kernel_initializer='random_uniform',
                       input_shape=(input_size,)))

#optimizer = keras.optimizers.SGD(learning_rate=0.05)
optimizer = tf.keras.optimizers.Adam()

num_episodes = 2000
discount_factor = 0.99

rList = []
for i in range(num_episodes):
    state = env.reset()
    rAll = 0
    done = False
    e = np.fmax(1 / ((i // 100) + 1), 0.05)
    print(i)

    while not done:
        with tf.GradientTape() as tape:
            q_p = model(one_hot(state))
            q = np.array(q_p)
            if np.random.rand(1) < e:
                action = env.action_space.sample()
            else:
                action = np.argmax(q_p)
            new_state, reward, done, _ = env.step(action)
            if done:
                q[0][action] = reward
            else:
                q[0][action] = reward + discount_factor * np.max(model(one_hot(new_state)))
            loss_value = loss_fn(q, q_p)
        grads = tape.gradient(loss_value, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

        rAll += reward
        state = new_state
    rList.append(rAll)

print("Success rate: " + str(sum(rList) / num_episodes))
~~~

그런데 위와 같이 국소적인 gradient 를 갖고 step 을 진행할 때마다, 실제로 optimize 한 action-value function 에 수렴하는가 하면 그렇지 않다고 한다. 그 이유는 다음과 같다.

- correlated samples : 국소적으로 모인 샘플들은 서로 correlation 을 갖고 있을 가능성이 높고, 그들은 전체 샘플들의 경향성을 잘 대표하지 못한다. (아마 그런 의미일 거야..)
- non-stationary target : gradient descent 를 통해서 target 에 맞도록 q 가 수정되고나면, 그 수정으로 인해 target 도 바뀌기 때문에 target 과의 격차가 다시 생기는 것.

위의 문제점들을 수정한 방법론이 **DQN** 이다.

핵심적인 아이디어는 다음과 같다.

- environment 속에서 action 을 취하면서 바로바로 학습하는 것이 아니라, (state, action, nextState, reward) 를 버퍼에 쌓아둔 이후에, 몇 개씩 샘플링해서 그것을 기반으로 학습시킨다.

- 서로 같은 Q 와 Q' 의 두 개 준비한다. 학습을 시킬 때 Q' 는 고정해두고, Q' 를 통해서 target 을 계산, Q 를 학습시킨다. 이후에 Q' 를 학습된 Q 로 초기화한다. 이 과정을 반복한다.