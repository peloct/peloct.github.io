<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JunWoo's Blog</title>
    <description>Hi! My name is JunWoo. I am a programmer highly interested in physically based animation or rendering (and everything categorized in graphics). I wanna share my knowledge, or take a note what I'm studying through this page. 
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 10 Sep 2020 10:33:21 +0900</pubDate>
    <lastBuildDate>Thu, 10 Sep 2020 10:33:21 +0900</lastBuildDate>
    <generator>Jekyll v4.1.1</generator>
    
      <item>
        <title>강화학습 5 (Q-Network 방법론)</title>
        <description>&lt;p&gt;Q-Table 방법론은 Environment 의 상태와 액션의 수가 적은 경우 사용할 수 있는 방법론이며, Table 이 커짐에 따라, action-value function 을 근사할 수 있는 또 다른 방법론이 필요해진다.&lt;/p&gt;

&lt;h2 id=&quot;q-network&quot;&gt;Q-Network&lt;/h2&gt;

&lt;p&gt;Q-Network 는 결국 상태를 입력으로 받으면 액션들의 점수(현재 상황에서 어떤 액션은 몇 점을 기대가능 등)를 반환하는 함수이다. 그리고 그것은 Neural Network 를 통해서 구현된다.&lt;/p&gt;

&lt;p&gt;현재 나의 이해로는 세상 어딘가에 optimal 한 action-value function 이 있다고 믿고, Q-Network 는 이 action-value function 을 비교적 적은 인자를 통해서 근사하는 것을 목적으로 하는 방법론이다.&lt;/p&gt;

&lt;p&gt;Neural Netwrok 은 결국 Weight 의 모임이고, 각 Weight, 입력에 대한 최종 값의 미분을 구할 수 있기 때문에 Gradient descent method 를 통해서 Optimizing 하는 것이 가능하다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;init Q with random weights

for episode from 1 to M do
    setup environment

    for t from 1 to T do
        p = Q(state)
        action = argmax(p)
        nextState, reward = environment.step(action)
        
        if isTerminal(nextState)
            y = reward
        else
            y = reward + gamma * max(Q(nextState))

        perform gradient descent step on (y - p[action])^2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;모든 상태에 대해서 각 상태의 Q 가 다음 상태의 최대 Q 와 같을 때, 이 Q 가 optimal 이라는 말일까? 잘 모르겠다. 어쨌든 가장 원하는 상태는 각 Q를 통해서 가장 좋다고 판단되는 선택을 했을 때의 Q 값이, 그 선택으로 인해 이동한 곳에서의 최대 Q 와 같은 경우인듯 하다.&lt;/p&gt;

&lt;p&gt;한 번 구현해본 코드.(잘 안된다. 잘못 구현했을 지도 모른다. Keras… 정신나갈거같아정신나갈거같아)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.keras&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.keras.layers&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduce_sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'FrozenLake-v0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;one_hot_table&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# model define
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;kernel_initializer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'random_uniform'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#optimizer = keras.optimizers.SGD(learning_rate=0.05)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;num_episodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;discount_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rList&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rAll&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GradientTape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;q_p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;new_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;discount_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;one_hot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainable_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply_gradients&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainable_weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;rAll&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_state&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Success rate: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;그런데 위와 같이 국소적인 gradient 를 갖고 step 을 진행할 때마다, 실제로 optimize 한 action-value function 에 수렴하는가 하면 그렇지 않다고 한다. 그 이유는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;correlated samples : 국소적으로 모인 샘플들은 서로 correlation 을 갖고 있을 가능성이 높고, 그들은 전체 샘플들의 경향성을 잘 대표하지 못한다. (아마 그런 의미일 거야..)&lt;/li&gt;
  &lt;li&gt;non-stationary target : gradient descent 를 통해서 target 에 맞도록 q 가 수정되고나면, 그 수정으로 인해 target 도 바뀌기 때문에 target 과의 격차가 다시 생기는 것.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;위의 문제점들을 수정한 방법론이 &lt;strong&gt;DQN&lt;/strong&gt; 이다.&lt;/p&gt;

&lt;p&gt;핵심적인 아이디어는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;environment 속에서 action 을 취하면서 바로바로 학습하는 것이 아니라, (state, action, nextState, reward) 를 버퍼에 쌓아둔 이후에, 몇 개씩 샘플링해서 그것을 기반으로 학습시킨다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;서로 같은 Q 와 Q’ 의 두 개 준비한다. 학습을 시킬 때 Q’ 는 고정해두고, Q’ 를 통해서 target 을 계산, Q 를 학습시킨다. 이후에 Q’ 를 학습된 Q 로 초기화한다. 이 과정을 반복한다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 09 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note13</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note13</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>강화학습 4 (Q-Table 방법론)</title>
        <description>&lt;h2 id=&quot;q-table-방법론&quot;&gt;Q-Table 방법론&lt;/h2&gt;

&lt;p&gt;Q-Table 방법론은 Environment 를 제한된 상태로 표현할 수 있고 액션 또한 한정되어 있는 경우, action-value function 를 학습시키는 방법론이다.&lt;/p&gt;

&lt;p&gt;Policy 는 action-value function 을 기반으로, 현재 상태에서 argmax 한 행동을 선택하는 것으로 한다. (Exploiting 을 위해 때때로 다른 행동을 하도록 랜덤값을 넣기도 한다.)&lt;/p&gt;

&lt;p&gt;action-value function 은 “어떤 상태에, 어떤 행동을 하면 최종적으로 몇 점을 기대할 수 있는가?” 를 말하기 때문에, 이를 별도의 테이블(Q-Table)에 저장한다고 하면 &lt;strong&gt;상태 수 x 액션 수&lt;/strong&gt; 사이즈의 테이블이 요구된다.&lt;/p&gt;

&lt;p&gt;아이디어는 여러 번의 꽤 많은 episode 를 거치면서 다음의 식으로 action-value function 을 개선하는 것이다.&lt;/p&gt;

\[Q[ curState, action ] = R + \gamma * \max_{a \in Action} Q[ nextState, a]\]

&lt;p&gt;\(R\) 는 방금 action 을 취함으로써 Environment 로부터 얻은 reward 이다.&lt;/p&gt;

&lt;p&gt;문제는 수식은 Environment 가 deterministic 한 경우에만 통한다. Environment 에 따라서 같은 상태에서 이전과 같은 action 을 취했을 때, 이전의 경험과는 다른 상태에 도달하는 Environment 또한 있다.&lt;/p&gt;

&lt;p&gt;이러한 경우에 위의 수식은 다음과 같이 바뀐다.&lt;/p&gt;

\[Q[ curState, action ] = (1 - \alpha) * Q[ curState, action ]\]

\[+ \alpha * (R + \gamma * \max_{a \in Action} Q[ nextState, a])\]

&lt;p&gt;요컨대 지금의 실패나 성공을 반드시 믿으며 이전의 경험을 날리지 않고, 이전의 경험을 조금씩 남기는 것이다. (\(\alpha\) 는 learning rate 라 하며 0부터 1사이의 값이다.)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;gym&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gym&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;make&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'FrozenLake-v0'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;render&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;observation_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_episodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5000&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;discount_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.99&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.85&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rList&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action_space&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;new_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;done&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;\
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;\
            &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;discount_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_state&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Success rate: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위는 Stochastic 한 Environment 인 FrozenLake 를 푸는 Q-Table Method 의 구현이다.&lt;/p&gt;
</description>
        <pubDate>Wed, 09 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note12</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note12</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>머신러닝</title>
        <description>&lt;h2 id=&quot;type-of-supervised-learning&quot;&gt;Type of supervised learning&lt;/h2&gt;

&lt;p&gt;score =&amp;gt; regression&lt;/p&gt;

&lt;p&gt;1 아니면 0 =&amp;gt; binary classification&lt;/p&gt;

&lt;p&gt;1, 2, 3, 4, 5 =&amp;gt; multilevel classification&lt;/p&gt;

&lt;p&gt;시간에 따른 시험점수같은 것을 학습 시킨다면 : regression model&lt;/p&gt;

&lt;p&gt;시간에 따라 합격, 불합격을 학습 시킨다면 : binary classification&lt;/p&gt;

&lt;p&gt;시간에 따라 A B D F : multilevel classification&lt;/p&gt;

&lt;h2 id=&quot;tensorflow-에서-tensor-정의&quot;&gt;Tensorflow 에서 Tensor 정의&lt;/h2&gt;

&lt;p&gt;rank : 몇 차원 array 인가?&lt;/p&gt;

&lt;p&gt;1차원이면 벡터, 2차원이면 행렬, 3차원이면 3차원 텐서…&lt;/p&gt;

&lt;p&gt;shape : 차원 별로의 size&lt;/p&gt;

&lt;p&gt;(2 x 2 행렬 인가? 3 x 5 x 2 x 7 tensor 인가? 등)&lt;/p&gt;

&lt;p&gt;type : 데이터 타입&lt;/p&gt;

&lt;h2 id=&quot;linear-regression&quot;&gt;Linear regression&lt;/h2&gt;

&lt;p&gt;Linear regression model 은 linear 한 모델이 데이터에 맞을 것이다 라고 가정한다.&lt;/p&gt;

&lt;p&gt;목적은 데이터들과의 오차의 제곱 평균을 최소화하는 직선 구하기. (기울기 및 y절편)&lt;/p&gt;

&lt;p&gt;가정함수(예측함수)는 다음과 같다.&lt;/p&gt;

\[H(x) = Wx + b\]

&lt;p&gt;여기서 &lt;strong&gt;cost(W, b) = 오차의 제곱 평균&lt;/strong&gt; 이 가장 낮아지게 하는 W, b 를 구하는 것이 목적이다.&lt;/p&gt;

&lt;p&gt;Tensorflow 는 수식을 graph 의 형태로 만들기에, Backpropagation 을 통해 원인에 의한 최종 함수의 변화(미분) 을 계산할 수 있다.&lt;/p&gt;

&lt;p&gt;전체 함수의 미분을 알면, Descent Gradient 도 가능하다.&lt;/p&gt;

&lt;p&gt;따라서 cost 함수를 Tensorflow 의 그래프로 표현하고 Descent Gradient 를 통해 optimize 하면 W, b 를 구하는 것이 가능하다.&lt;/p&gt;

&lt;h2 id=&quot;neural-network&quot;&gt;Neural Network&lt;/h2&gt;

&lt;p&gt;Neural Network 도 결국 input 들과 weight 들에 대한 최종 output 의 함수이며, 따라서 Optimize 가 가능하다.&lt;/p&gt;

&lt;p&gt;Activation function 이란 neural 의 활성화, 비활성화 정도를 나타내는 함수로, Linear Combination 된 벡터에 Component wise 하게 적용된 이후, 다음 Layer 의 입력으로 들어가게 된다.&lt;/p&gt;

&lt;p&gt;Backpropagation 을 이용하는 것은 좋은데, Network 가 깊어지면 깊어질수록 출력으로부터 멀리 있는 변수의 영향이 작아진다.(Vasnishing Gradient) 이때 각 레이어의 Activation function 으로 Sigmoid 를 사용하지 않고 ReLU 를 사용하면 좋다.(왜?)
(ReLU 외에도 여러 Activation function 들이 존재한다.)&lt;/p&gt;

&lt;p&gt;Overfitting : 데이터를 학습시키다보면 학습 데이터 내에서의 정확도는 오르지만, 점차 학습하지 않은 다른 데이터에 대해서는 정확도가 낮아지게 되는 현상. =&amp;gt; Regulation 이라는 과정을 통해 완화할 수 있다.&lt;/p&gt;

&lt;p&gt;Regulation : weignt 에 큰 값이 들어가지 않게 한다.
Dropout : 학습시킬때 랜덤하게 Layer 와 Layer 사이에 연결을 일부 끊어서 학습시킨다. (제한된 정보만으로 목적을 이루도록 학습시키는 것과 유사) (Overfitting 을 완화하는 방법론 중 하나)
Ensemble : 여러 트레이닝 셋으로 각각 학습된 모델을 이용한다. 각 모델에서 내뱉은 값을 조합해서 최종결론을 내린다.&lt;/p&gt;

&lt;h2 id=&quot;nn-이리저리-만들기&quot;&gt;NN 이리저리 만들기&lt;/h2&gt;

&lt;p&gt;Fast forward : 어떤 레이어의 결과를 바로 다음 레이어에 전달하는 것 외에도, 그 이후 어떤 시점에 집어넣는다. (합연산)&lt;/p&gt;

&lt;p&gt;Split &amp;amp; Merge : 전체 NN을 중간에 두갈래로 나눴다가 합친다. 처음 입력단부터 이미 여러 NN 갈래로 시작한다.&lt;/p&gt;

&lt;p&gt;Recurrent Network : 뭔가 노드를 그리드 형태로 만든 다음, 아래에서 위로 뿐만 아니라 왼쪽에서 오른쪽으로.&lt;/p&gt;

&lt;p&gt;어쨌든 별에 별 이상한 NN 형태 만들기가 있다. 해봤더니 잘되더라? 하는 느낌.&lt;/p&gt;

&lt;h2 id=&quot;cnn-convolutional-neural-networks&quot;&gt;CNN (Convolutional Neural Networks)&lt;/h2&gt;

&lt;p&gt;기본은 입력을 바로 사용하는 것이 아니라, Filter 라는 것을 이용해서 입력의 부분부분에 filter 를 적용, filter 의 결과를 이용하는 것이다. 이 과정을 Feature extraction 이라고 부른다.&lt;/p&gt;

&lt;p&gt;Convolutional Layer (CL)&lt;/p&gt;

&lt;p&gt;filter 를 통해서 convolution 하여 그 결과를 내보내는 레이어다.&lt;/p&gt;

&lt;p&gt;filter 적용 규칙의 명시&lt;/p&gt;

&lt;p&gt;shape : 필터의 사이즈
stride : 몇 칸씩 움직이며 적용할 것인가?
padding : 입력에 줄 padding (convolution 결과의 shape 이 원본 입력의 shape 과 같게 하는 등의 목적을 위해)
필터의 개수 : 하나의 입력으로부터 몇 가지의 서로 다른 output 을 낸다.&lt;/p&gt;

&lt;p&gt;Pooling Layer (PL)&lt;/p&gt;

&lt;p&gt;Pooling 은 입력을 읽고 모종의 규칙으로 샘플링한 결과를 내보내는 레이어다. (그런데 이게 Convolution 아닌가?)&lt;/p&gt;

&lt;p&gt;CNN 은 대강 다음과 같은 구조이다.&lt;/p&gt;

&lt;p&gt;입력 - CL1 - PL1 - CL2 - PL2 - … - PLn - 일반적인 DNN&lt;/p&gt;

&lt;h2 id=&quot;rnn-recurrent-neural-network&quot;&gt;RNN (Recurrent Neural Network)&lt;/h2&gt;

&lt;p&gt;순차적으로 발생하는 사건 등을 학습시키기에 좋다.&lt;/p&gt;

</description>
        <pubDate>Wed, 09 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note11</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note11</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>강화학습 2</title>
        <description>&lt;p&gt;Markov Decision Process 문제를 해결한다고 하자.(즉 environment 가 fully observable, state 가 다음 state 를 결정하기에 충분한 정보를 갖는다.) 강화학습을 하기 위해선, Environment 를 Markov Decision Process 로 모델링하는 것이 중요.&lt;/p&gt;

&lt;h2 id=&quot;max-와-argmax&quot;&gt;max 와 argmax&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;max 함수&lt;/strong&gt;는 어떤 집합 내에서 가장 큰 값을 찾는 함수이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;argmax 함수&lt;/strong&gt;는 인자에 대해서 집합 내에 한 원소가 결정되고, 집합 내에서 가장 큰 원소를 찾았을 때, 그 원소에 대응되는 인자를 반환하는 함수이다. (요컨대 max 가 되게 하는 argument 를 찾는 함수)&lt;/p&gt;

&lt;h2 id=&quot;state-transition-matrix-의-정의&quot;&gt;State Transition Matrix 의 정의&lt;/h2&gt;

&lt;p&gt;총 n 개의 state \(s_1, s_2, \cdots , s_n\) 가 있다고 했을 때,&lt;/p&gt;

\[P_{ij} = P[ S_{t+1} = s_j \vert S_t = s_i ]\]

&lt;p&gt;이것은 Random 하게 상태가 변경되는 경우에 각 state 에서 다른 state 로 이동할 확률을 명시한 행렬이다.&lt;/p&gt;

&lt;h2 id=&quot;markov-process&quot;&gt;Markov Process&lt;/h2&gt;

&lt;p&gt;Markov Process 는 다음과 같이 명시할 수 있다.&lt;/p&gt;

\[&amp;lt; S, P &amp;gt;\]

&lt;p&gt;\(S\) : 모든 상태 집합&lt;/p&gt;

&lt;p&gt;\(P\) : State Transition Matrix&lt;/p&gt;

&lt;p&gt;이 위에서 랜덤하게 State 와 State 를 오간다.&lt;/p&gt;

&lt;h2 id=&quot;episode&quot;&gt;Episode&lt;/h2&gt;

&lt;p&gt;Episode 의 정의는 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;어느 state 에서 시작해서 마지막 state 까지 가는 과정&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;샘플링이라는 것은 확률변수로부터 이벤트가 발생한다는 것을 말하는데, 어떤 확률분포를 갖는 확률변수의 한 사건을 추출한다는 것. 이러한 측면에서 Markov Process 로부터 샘플링을 수행한 것을 episode 라고 볼 수 있겠다.&lt;/p&gt;

&lt;h2 id=&quot;markov-reward-process&quot;&gt;Markov Reward Process&lt;/h2&gt;

&lt;p&gt;Markov Reward Process 는 다음과 같이 명시할 수 있다.&lt;/p&gt;

\[&amp;lt; S, P, R, \gamma &amp;gt;\]

&lt;p&gt;일반적인 Markov Process 와는 달리 각 상태에 점수가 부여되어 있으며, 해당 상태에 진입할 때마다&lt;/p&gt;

&lt;p&gt;\(S\) : 모든 상태 집합&lt;/p&gt;

&lt;p&gt;\(P\) : State Transition Matrix&lt;/p&gt;

&lt;p&gt;\(R\) : reward function&lt;/p&gt;

&lt;p&gt;\(\gamma\) : discount factor&lt;/p&gt;

&lt;p&gt;이때 reward function 이란 다음과 같이 정의된다.&lt;/p&gt;

\[R_s \equiv E[ Re_{t+1} | S_t = s]\]

&lt;p&gt;어떤 상태에 있을 때, 다음으로 얻을 Reward(\(Re\)) 의 기대값을 말한다.&lt;/p&gt;

&lt;h2 id=&quot;return&quot;&gt;Return&lt;/h2&gt;

&lt;p&gt;강화학습은 return \(G_t\) 을 maximize 하는 프로세스이다.&lt;/p&gt;

&lt;p&gt;이때 return 은 다음과 같이 정의된다.&lt;/p&gt;

\[G_t \equiv Re_{t+1} + \gamma Re_{t + 2} + \gamma^2 Re_{t + 3} + \cdots\]

&lt;p&gt;각 보상 Reward(\(Re\)) 별로 앞에 \(\gamma\) 가 붙어서 미래의 보상에는 좀 더 낮은 가중치를 준다.&lt;/p&gt;

&lt;p&gt;discount factor (\(\gamma\)) 를 넣는 이유는 수학적으로 편리하기 때문이라고 한다. 수렴성이 증명된다나.&lt;/p&gt;

&lt;p&gt;Markov reward process 가 최종적으로 terminate 될 수 있다면 gamma 가 1 이어도 될 수도 있다.&lt;/p&gt;

&lt;h2 id=&quot;mrp-의-value-function&quot;&gt;MRP 의 Value function&lt;/h2&gt;

&lt;p&gt;MRP 에서의 value function 은 return 의 기대값.&lt;/p&gt;

&lt;p&gt;episode 를 샘플링하면, 거기에서 return 이 결정되니까 이 return 도 확률변수이다.&lt;/p&gt;

\[v(s) = E[ G_t \vert S_t = s ]\]

&lt;h2 id=&quot;bellman-equation-for-mrp&quot;&gt;Bellman Equation for MRP&lt;/h2&gt;

&lt;p&gt;벨만 방정식이라고 부른다.&lt;/p&gt;

&lt;p&gt;위의 정의에 따라서 어떤 상태 \(s\) 에서의 value function 과 다음 상태들 \(s'\) 의 value function 사이에 무슨 관계가 있는지를 알려준다.&lt;/p&gt;

\[v(s) = E[ G_t \vert S_t = s ]\]

\[= E[ Re_{t+1} + \gamma Re_{t + 2} + \cdots \vert S_t = s ]\]

\[= E[ Re_{t+1} + \gamma G_{t+1} \vert S_t = s ]\]

\[= E[ Re_{t+1} + \gamma v(S_{t + 1}) \vert S_t = s ]\]

\[= R_s + \gamma \sum_{s' \in S}P_{ss'}v(s')\]

&lt;p&gt;이때,&lt;/p&gt;

\[V^T = (v(1), v(2), ... , v(n))\]

\[R^T = (R_1, R_2, ... , R_n)\]

&lt;p&gt;이라 하면,&lt;/p&gt;

\[V = R + \gamma PV\]

\[(I - \gamma P) V = R\]

\[V = (I - \gamma P)^{-1} R\]

&lt;p&gt;즉, MRP 에서는 value function 이 바로 계산되어진다.&lt;/p&gt;

&lt;p&gt;(하지만 계산복잡도가 O^3 이기 때문에, 큰 MRP 문제에서는 Dynamic programming, Monte-Carlo evaluation, Temporal-Difference Learning 을 사용한다.)&lt;/p&gt;

&lt;h2 id=&quot;markov-decision-process&quot;&gt;Markov Decision Process&lt;/h2&gt;

&lt;p&gt;Markov Reward Process 는 다음과 같이 명시할 수 있다.&lt;/p&gt;

\[&amp;lt; S, A, P, R, \gamma &amp;gt;\]

&lt;p&gt;\(S\) : 모든 상태 집합&lt;/p&gt;

&lt;p&gt;\(A\) : action 의 집합&lt;/p&gt;

&lt;p&gt;\(P\) : State Transition Matrix&lt;/p&gt;

&lt;p&gt;\(R\) : reward function&lt;/p&gt;

&lt;p&gt;\(\gamma\) : discount factor&lt;/p&gt;

&lt;p&gt;이때 \(P\) 는 MP, MRP 와는 살짝 변형된다.&lt;/p&gt;

&lt;p&gt;\(P_{ss'}^a \equiv\) action a 를 했을 때, state 가 s 에서 s’ 로 바뀔 확률이다. 임의의 상태에서 어떤 액션을 했을 때, 이동한 상태가 항상 일정하다면 이를 &lt;strong&gt;Deterministic&lt;/strong&gt;, 확률적으로 다른 상태로 이동하기도 한다면 &lt;strong&gt;Stochastic&lt;/strong&gt; 하다라고 한다.&lt;/p&gt;

&lt;p&gt;또한 \(R\) 도 살짝 변형된다.&lt;/p&gt;

&lt;p&gt;\(R_s^a \equiv\) state s 에서 action a 를 수행했을 때 얻는 \(Re_{t + 1}\) 의 기댓값&lt;/p&gt;

&lt;p&gt;이번에는 Action 이라는 개념이 추가되어서, Policy 에 따라 다른 action 을 취할 수 있는 가능성을 넣었다. MRP 에서는 랜덤프로세스였지만, MDP 에서는 어떤 액션을 수행할지 선택하는 것이다. 그래서 어떤 policy 를 갖고 액션을 수행할지가 중요하다.&lt;/p&gt;

&lt;p&gt;만약 Action 을 하면 확률적으로 다른 state 로 간다.&lt;/p&gt;

&lt;p&gt;우선 Agent 가 사용하는 policy 를 수학적으로 다음과 같이 모델링한다.&lt;/p&gt;

\[\pi(a\vert s) = P[ A_t = a \vert S_t = s ]\]

&lt;p&gt;MDP 에서 어떤 policy 를 갖고 움직일 때, 만약 policy 가 고정이라면, policy 또한 위의 함수처럼 확률적으로 Action을 결정하기에, MDP 를 MRP 로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;이때 MRP 의 명시는 다음과 같이 된다.&lt;/p&gt;

\[&amp;lt; S, P^\pi, R^\pi, \gamma &amp;gt;\]

&lt;p&gt;\(P^\pi\) 는 다음과 같이 정의된다.&lt;/p&gt;

\[P_{s,s'}^\pi \equiv \sum_{a \in A} \pi(a \vert s)P_{ss'}^a\]

&lt;p&gt;\(R^\pi\) 는 다음과 같이 정의된다.&lt;/p&gt;

\[R_s^\pi \equiv \sum_{a \in A} \pi(a \vert s)R_s^a\]

&lt;p&gt;state 의 변화만을 본다면, \(&amp;lt; S, P^\pi &amp;gt;\) 로 명시된 MP 가 된다.&lt;/p&gt;

&lt;h2 id=&quot;mdp-의-value-function&quot;&gt;MDP 의 Value function&lt;/h2&gt;

&lt;p&gt;state-value function \(v_\pi(s)\) 는 policy \(\pi\) 를 따라서 끝까지 했을 때, return 의 기대값으로 정의된다.&lt;/p&gt;

&lt;p&gt;action-value function \(q_\pi(s, a)\) 는 현재 state s 에서 행동 a 를 했을 때, policy \(\pi\) 를 따랐을 경우의 return 의 기대값. 이것의 좋은 점은 현재 상태가 있을때, 어떤 행동을 하면 좋냐? 라는 질문에 대한 실질적인 답변을 해준다는 점이다.&lt;/p&gt;

&lt;h3 id=&quot;bellman-expextation-equation-for-mdp&quot;&gt;Bellman Expextation Equation for MDP&lt;/h3&gt;

\[v_\pi(s) = E_\pi[ Re_{t+1} + \gamma v_\pi(S_{t + 1}) \vert S_t = s ]\]

\[q_\pi(s, a) = E_\pi[ Re_{t+1} + \gamma q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a ]\]

&lt;p&gt;v 와 q 사이의 관계는 다음과 같다.&lt;/p&gt;

\[v_\pi(s) = \sum_{a \in A}\pi(a \vert s)q_\pi(s, a)\]

\[q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s')\]

&lt;p&gt;따라서&lt;/p&gt;

\[v_\pi(s) = \sum_{a \in A}\pi(a \vert s)(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s'))\]

\[q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a ( v_\pi(s') = \sum_{a' \in A}\pi(a' \vert s')q_\pi(s', a') )\]

&lt;p&gt;Bellman Expextation Equation 의 Matrix Form 은 다음과 같다.&lt;/p&gt;

\[v_\pi = R^\pi + \gamma P^\pi v_\pi\]

\[v_\pi = (I - \gamma P^\pi)^{-1} R^\pi\]

&lt;h2 id=&quot;optimal-value-function&quot;&gt;Optimal value function&lt;/h2&gt;

&lt;p&gt;optimal state-value function \(v_{*}(s)\) 는 다음을 말한다.&lt;/p&gt;

\[v_{*}(s) = \max_\pi v_\pi(s)\]

\[q_{*}(s, a) = \max_\pi q_\pi(s, a)\]

&lt;p&gt;optimal value function 을 알 때, &lt;strong&gt;MDP가 풀렸다.&lt;/strong&gt; 라고 한다.&lt;/p&gt;

&lt;h2 id=&quot;optimal-policy&quot;&gt;Optimal policy&lt;/h2&gt;

&lt;p&gt;이 경우 policy 와 policy 는 partial ordering 을 갖는다. (집합론으로 치면, ordering 이 가능한 페어와 불가능한 페어가 있다는 것)&lt;/p&gt;

\[\pi \geq \pi' \quad if \quad v_\pi(s) \geq v_{\pi'}(s), \forall s\]

&lt;h2 id=&quot;mdp-에-대한-theorem&quot;&gt;MDP 에 대한 Theorem&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;모든 다른 policy 에 대해서 좋거나 같은 optimal policy 가 존재한다.&lt;/li&gt;
  &lt;li&gt;
\[v_{\pi*}(s) = v_{*}(s)\]
  &lt;/li&gt;
  &lt;li&gt;
\[q_{\pi*}(s, a) = q_{*}(s, a)\]
  &lt;/li&gt;
  &lt;li&gt;optimal policy 는 \(q_{*}(s, a)\) 로부터 구해질 수 있다.
\(\pi_{ *}(a \vert s) = 1 \quad if \quad a = \max_{a \in A} q_{ *}(s, a) \quad else \quad 0\)&lt;/li&gt;
  &lt;li&gt;어떤 MDP 에도 언제나 deterministic optimal policy 가 있다.&lt;/li&gt;
  &lt;li&gt;\(q_{*}(s, a)\) 를 알면, 바로 optimal policy 를 얻는다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bellman-optimality-equation-for-mdp&quot;&gt;Bellman Optimality Equation for MDP&lt;/h3&gt;

\[v_{*}(s) = \max_{a \in A}q_{ *}(s, a)\]

\[q_{*}(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{ *}(s')\]

\[v_{*}(s) = \max_{a \in A}( R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{ *}(s'))\]

&lt;p&gt;max 로 인해서 linear equation 이 아니기에, 바로 풀 수 없다. (no closed form solution)&lt;/p&gt;

&lt;p&gt;이를 풀기 위한 방법론들은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Value Iteration&lt;/li&gt;
  &lt;li&gt;Policy Iteration&lt;/li&gt;
  &lt;li&gt;Q-learning&lt;/li&gt;
  &lt;li&gt;Sarsa&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 08 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note9</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note9</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>(확률과 통계) 5. 공분산</title>
        <description>&lt;h2 id=&quot;두-확률변수-공분산의-정의&quot;&gt;두 확률변수 공분산의 정의&lt;/h2&gt;

&lt;p&gt;공분산은 다음의 질문에 답하기 위한 수치이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;두 확률변수 X, Y 가 있을때, X가 커지면 Y도 커지거나 X가 커지면 Y도 작아지는 성질이 있는가?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;위의 질문을 수학적으로 모델링하면 다음과 같다.&lt;/p&gt;

\[Cov[ X, Y ] \equiv E[(X - \mu)(Y - \nu)]\]

&lt;p&gt;만약 X 가 증가함에 따라 Y 가 증가하거나, X 가 감소함에 따라 Y 가 감소하면, Cov 의 값은 0 보다 크다. (양의 상관관계)&lt;/p&gt;

&lt;p&gt;만약 X 가 증가함에 따라 Y 가 감소하거나, X 가 감소함에 따라 Y 가 증가하면, Cov 의 값은 0 보다 작다. (음의 상관관계)&lt;/p&gt;

&lt;p&gt;만약 그 무엇도 아니면 X, Y 는 서로 무상관이라고 한다.&lt;/p&gt;

&lt;p&gt;참고로 X 와 Y 가 &lt;strong&gt;독립&lt;/strong&gt;이라면, \(Cov[ X, Y ] = 0\) 이다.&lt;/p&gt;

&lt;p&gt;Cov 의 연산 성질은 다음과 같다.&lt;/p&gt;

\[Cov[ X, Y ] = Cov[ Y, X ]\]

\[Cov[ X, X ] = V[ X ]\]

\[Cov[ aX, bY ] = abCov[ X, Y ] \cdots (1)\]

\[Cov[ X + a, Y + b ] = Cov[ X, Y ]\]

&lt;p&gt;그런데 위의 \((1)\) 파트를 보면, Cov 값이 X, Y 앞에 붙는 상수에 따라 그 값이 변동될 수 있음을 알 수 있다. 이는 달리 말하면 Cov 자체는 대략적인 상관관계를 알 수는 있어도, 어느 정도 상관관계가 있는지의 지표가 되긴 어렵다는 것을 의미한다.&lt;/p&gt;

&lt;p&gt;따라서 항상 그렇듯이 단위를 바꿔서 정규화를 시켜줘야한다.&lt;/p&gt;

&lt;p&gt;이때 단위는 표준편차이다.&lt;/p&gt;

\[X' = { X \over \sigma_X }, Y' = { Y \over \sigma_Y }\]

\[\rho_{XY} = Cov[ X', Y' ] = { Cov[ X, Y ] \over {\sigma_X\sigma_Y} }\]

&lt;p&gt;이를 &lt;strong&gt;상관계수&lt;/strong&gt; 라고 부른다.&lt;/p&gt;

&lt;p&gt;상관계수에는 다음의 성질이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;-1 부터 1 사이의 값을 갖는다. \(\cdots (1)\)&lt;/li&gt;
  &lt;li&gt;상관계수가 1에 가까울수록 (X, Y)는 올라가는 직선에 가깝게 위치한다.&lt;/li&gt;
  &lt;li&gt;상관계수가 -1에 가까울수록 (X, Y)는 내려가는 직선에 가깝게 위치한다.&lt;/li&gt;
  &lt;li&gt;X, Y가 서로 독립이면 상관계수는 0이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(1) 을 증명해보자면,&lt;/p&gt;

&lt;p&gt;\((X, Y) = (a_i, b_i)\) 일 확률이 \(p_i\), 각 확률변수의 기댓값을 \(\mu, \nu\) 라고 하자. 이 때&lt;/p&gt;

\[a^T \equiv ( \sqrt{p_1} * (a_1 - \mu), \sqrt{p_2} * (a_2 - \mu), \cdots, \sqrt{p_n} * (a_n - \mu) )\]

\[b^T \equiv ( \sqrt{p_1} * (b_1 - \nu), \sqrt{p_2} * (b_2 - \nu), \cdots, \sqrt{p_n} * (b_n - \nu) )\]

&lt;p&gt;라고 하면,&lt;/p&gt;

\[\rho_{XY} = { { a \cdot b } \over { \lVert a \rVert \lVert b \rVert } }\]

&lt;p&gt;그런데 꼭 위의 상관계수를 이용해서 분석해보았을 때, 0이라고 해서 두 확률변수가 서로 아무런 상관관계도 없다라고 생각하는 것은 문제가 있다. (X, Y) 의 샘플링 분포가 만약 반지모양을 이룬다고 하면 상관관계는 0이지만, 확실하게 서로 영향을 주고받는 변수라는 것을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;공분산행렬&quot;&gt;공분산행렬&lt;/h2&gt;

&lt;p&gt;여러 확률변수 \(X_1, X_2, \cdots, X_n\) 를 다음과 같이 벡터로 표현한다고 하자.&lt;/p&gt;

\[X^T = (X_1, X_2, \cdots, X_n)\]

\[E[ X ] = \mu = (E[ X_1 ], E[ X_2 ], \cdots, E[ X_n ])\]

&lt;p&gt;확률변수 \(X_1, X_2, \cdots, X_n\) 의 공분산행렬은 다음과 같이 정의된다.&lt;/p&gt;

\[M_{ij} \equiv Cov[ X_i, X_j ] = E[ (X - \mu)(X - \mu)^T ]\]

&lt;p&gt;이 때, 확률변수벡터 \(X\) 의 분산은 공분산행렬로 정의된다.&lt;/p&gt;

\[V[ X ] \equiv E[ (X - \mu)(X - \mu)^T ]\]

&lt;p&gt;Cov 의 성질에 의해 공분산행렬은&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;대칭행렬&lt;/li&gt;
  &lt;li&gt;모든 원소가 0 보다 크거나 같다.&lt;/li&gt;
  &lt;li&gt;대각성분은 각 확률변수의 분산&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉, positive symmetric 한 행렬이다.
positive symmetric 한 행렬은 다음의 성질을 갖는다. (아마? 기억하기로는…)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;eigen value 들이 실수이다.&lt;/li&gt;
  &lt;li&gt;eigen vector 끼리 orthogonal 하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;확률변수벡터 \(\vec{X}\) 의 기댓값, 분산에 대해 다음의 연산이 성립한다.&lt;/p&gt;

&lt;p&gt;(\(c\) 는 스칼라 상수, \(\vec{C}\) 는 상수 벡터, \(\vec{Y}\) 는 확률변수벡터)&lt;/p&gt;

\[E[ \vec{C} ] = \vec{C}\]

\[E[ c\vec{X} ] = cE[ \vec{X} ]\]

\[E[ \vec{X} + \vec{Y} ] = E[ \vec{X} ] + E[ \vec{Y} ]\]

\[E[ \vec{X} + \vec{C} ] = E[ \vec{X} ] + \vec{C}\]

\[E[ \vec{C} \cdot \vec{X} ] = \vec{C} \cdot E[ \vec{X} ]\]

\[V[ \vec{X} ] = E[ \vec{X}\vec{X}^T ] - E[ \vec{X} ]E[ \vec{X} ]^T\]

\[V[ c\vec{X} ] = c^2V[ \vec{X} ]\]

\[V[ \vec{C} \cdot \vec{X} ] = \vec{C}^T V[ \vec{X} ] \vec{C} \cdots (1)\]

&lt;p&gt;(\(R, S\) 는 확률변수행렬, \(A\) 는 상수행렬)&lt;/p&gt;

\[E[ A\vec{X} ] = AE[ \vec{X} ]\]

\[E[ AR ] = AE[ R ]\]

\[E[ RA ] = E[ R ]A\]

\[E[ cR ] = cE[ R ]\]

\[E[ R + A ] = E[ R ] + A\]

\[E[ R + S ] = E[ R ] + E[ S ]\]

\[E[ A ] = A\]

\[V[ A\vec{X} ] = AV[ \vec{X} ]A^T\]

&lt;p&gt;위의 (1) 에서, 만약 \(\vec{C}\) 가 단위벡터라면, \(V[ \vec{C} \cdot \vec{X} ]\) 는 확률변수벡터 \(\vec{X}\) 가 \(\vec{C}\) 방향으로 갖는 분산을 의미한다. 이때 공분산행렬을 다음과 같이 대각화했다고 하자.&lt;/p&gt;

\[V[ \vec{X} ] = RDR^T\]

&lt;p&gt;여기서 \(D\) 는 공분산행렬의 eigen value 를 정렬하여 (1, 1) 에 가장 큰 절댓값을 갖는 eigen value 가 오게 한 대각행렬이다. 또한 \(R\) 은 \(D\) 의 각 eigen value 에 대응하는 eigen vector 를 normalize 하여 열벡터로 채워넣은 행렬이다. 공분산행렬은 positively symmetric 하기 때문에 eigen value 가 \(X\) 의 차원수만큼 있다면, \(R\) 의 각 열은 orthonormal 하다.&lt;/p&gt;

\[V[ \vec{C} \cdot \vec{X} ] = \vec{C}^TRDR^T\vec{C}\]

&lt;p&gt;의 절대값이 최대가 되게 하는 \(\vec{C}\) 의 방향은 \(R\) 의 첫번째 열과 평행한 방향을 가리킨다. 또한 그렇게 결정된 \(\vec{C}\) 와 수직하면서 동시에 위 식의 절대값이 최대가 되게 하는 두 번째 벡터는 \(R\) 의 두번째 열과 평행한 방향을 가리킨다.&lt;/p&gt;

&lt;p&gt;물리엔진에서 어떤 물체에 딱 들어맞는 Oriented Box 를 만들 때 이 성질을 기반으로 한다. 물체의 convex hull 을 구한 뒤 convex hull 의 각 정점으로 공분산행렬 (covariance matrix) 를 구한다. 이 공분산행렬의 eigen vector 를 가지고 Box 를 회전시켜 물체에 씌우게 되면, 물체에 딱 fit 하는 Oriented Box 를 계산하는 것이 가능하다. 항상 covariance matrix 가 어떻게 동작하는 것인지 궁금했는데, 이렇게 익히게 되니 기쁘다.&lt;/p&gt;

&lt;h2 id=&quot;확률변수벡터의-확률밀도함수&quot;&gt;확률변수벡터의 확률밀도함수&lt;/h2&gt;

&lt;p&gt;확률변수벡터의 확률밀도함수는 다음과 같이 표시한다.&lt;/p&gt;

\[f_X(x) = f_{X_1X_2...X_n}(x_1, x_2, ... , x_n)\]

&lt;p&gt;확률은 다음과 같이 구해진다.&lt;/p&gt;

&lt;p&gt;\(X\) 가 \(D\) 안에 들어갈 확률은&lt;/p&gt;

\[P(X \in D) = \int_{D} f_X(x) dx\]

&lt;h2 id=&quot;여러-확률변수벡터-사이의-독립&quot;&gt;여러 확률변수벡터 사이의 독립&lt;/h2&gt;

\[f_{XYZ}(x, y, z) = f_{X_1, ... , X_a, Y_1, ... , Y_b, Z_1, ... , Z_c}(x_1, ... , x_a, y_1, ... , y_b, z_1, ... , z_c)\]

&lt;p&gt;위와 같을 때,&lt;/p&gt;

\[f_{XYZ}(x, y, z) = f_X(x)f_Y(y)f_Z(z)\]

&lt;p&gt;가 성립하면 \(X, Y, Z\) 는 서로 독립적이다. (그냥 실수확률변수에서 2개의 실수확률변수가 서로 독립인지 따졌는데, 그것의 일반화라고 생각하면 편하다.)&lt;/p&gt;

</description>
        <pubDate>Tue, 08 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note8</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note8</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>강화학습 3</title>
        <description>&lt;p&gt;Dynamic Programming 은 MDP 에 대한 모든 지식들을 전부 알고 있는 경우에 Planning 을 위해 쓰인다. ( 상태, State Translation Matrix, Reward Function 등 )&lt;/p&gt;

&lt;p&gt;Prediction 은 MDP 와 policy 가 주어졌을 때, Value function 을 맞추는 문제&lt;/p&gt;

&lt;p&gt;Control 은 MDP 가 주어졌을 때, optimal value function 과 optimal policy 를 찾는 문제&lt;/p&gt;

&lt;h2 id=&quot;iterative-policy-evaluation&quot;&gt;Iterative Policy Evaluation&lt;/h2&gt;

&lt;p&gt;해당 policy 를 따랐을때 value function 을 계산하는 방법론.&lt;/p&gt;

&lt;p&gt;synchronous backup.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;v = [num of state]
for k + 1 (아마 만족할때까지)
    foreach s of State
        update(v[s]) from v[s'] where s' is successor state of s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

\[v_{k + 1}(s) = \sum_{a \in A} \pi(a \vert s)(R_s^a + \gamma \sum_{s' \in S}P_{ss'}a v_k(s'))\]

&lt;p&gt;위와 같이 얻어낸 value function 은 현재 policy 를 평가한 결과이다. (즉 prediction 문제를 푼 것) 그런데 이 때, value function 을 가지고 Greed 하게 선택을 하면 이는 더 나은 policy 가 된다. 현재 policy 가 무엇인지 상관없이, 더 나은 policy 가 만들어지는 것이다.&lt;/p&gt;

&lt;p&gt;주어진 Policy 에 대해서&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Policy 를 평가하여 value function 을 만든다.&lt;/li&gt;
  &lt;li&gt;만들어진 value function 에 대해서 greed 하게 행동하는 policy 를 만든다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;위의 과정을 반복하면 policy 가 점점 개선된다.
\(\pi\) 는 점차 \(\pi_{*}\) 에 도달한다.&lt;/p&gt;

&lt;p&gt;이 방법론을 &lt;strong&gt;Policy Iteration&lt;/strong&gt; 라고 한다.&lt;/p&gt;

&lt;p&gt;greedy 하게 행동하는 policy 는 deterministic policy 일 것이므로, 초기에 deterministic policy 가 주어졌다고 가정하고 증명을 시작한다.&lt;/p&gt;

&lt;p&gt;(deterministic 이란 policy 가 어떤 state 에서 하는 행동이 하나로 결정되어있음을 의미한다.)&lt;/p&gt;

&lt;p&gt;그러한 policy 를 \(\pi\) 라고 하면,&lt;/p&gt;

\[\pi'(s) = \max_{a \in A} q_\pi(s, a)\]

&lt;p&gt;로 새로운 policy 를 만드는 것으로 개선하는 것이 가능할 것이다.&lt;/p&gt;

\[q_\pi(s, \pi'(s)) = \max_{a \in A} q_\pi(s, a) \ge \max q_\pi(s, \pi(s)) = v_\pi(s)\]

\[v_\pi(s) \le q_\pi(s, \pi'(s)) =\]

&lt;p&gt;// 49 : 19&lt;/p&gt;

&lt;h2 id=&quot;modified-policy-iteration&quot;&gt;Modified Policy Iteration&lt;/h2&gt;

&lt;p&gt;앞서 policy evaluation 을 value function 을 구할 때까지 돌려야 하는가? 하는 질문을 할 수 있는데, 적당히 3번을 하고 improve 를 진행해도 상관없다고 한다.&lt;/p&gt;

&lt;h2 id=&quot;principle-of-optimality&quot;&gt;Principle of Optimality&lt;/h2&gt;

&lt;p&gt;??&lt;/p&gt;

&lt;h2 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h2&gt;

&lt;p&gt;Policy Iteration 은 Policy 가 주어진 상태에서 Policy Evaluation 과 Greed Policy 를 통한 Policy 개선을 반복하는 것이었는데,&lt;/p&gt;

&lt;p&gt;Value Iteration 은 Policy 가 없는 경우에 다음을 반복해서 optimal value function 을 찾는 과정이다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;v = [num of state]
for k + 1 (아마 만족할때까지)
    foreach s of State
        update(v[s]) from v[s'] where s' is successor state of s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

\[v(s) = max_{a \in A}(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v(s'))\]

&lt;p&gt;을 이용해서 value function 을 업데이트한다.&lt;/p&gt;

&lt;h2 id=&quot;여러-최적화-테크닉들&quot;&gt;여러 최적화 테크닉들&lt;/h2&gt;

&lt;h3 id=&quot;in-place-dynamic-programming&quot;&gt;In-Place Dynamic Programming&lt;/h3&gt;

&lt;p&gt;위의 방법론들은 모든 상태 s 에 대해서 value function 을 업데이트할 때, 이전 iteration 에서의 value function 을 별도로 저장하고, 새로운 value function 을 위의 식들을 통해서 계산하는 식으로 동작했다.&lt;/p&gt;

&lt;p&gt;in-place dp 는 그냥 별도로 저장 안 하고, 바뀐 애를 참조한다면 그냥 바뀐 애를 이용해서 새로운 값을 계산하는 방식이다.&lt;/p&gt;

&lt;h3 id=&quot;prioritised-sweeping&quot;&gt;Prioritised Sweeping&lt;/h3&gt;

\[\left\vert max_{a \in A}(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v(s')) - v(s) \right\vert\]

&lt;p&gt;위와 같이 정의되는 Bellman error 가 큰 애들을 먼저 업데이트 한다.&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note10</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note10</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>(확률과 통계) 4. 연속값의 확률분포</title>
        <description>&lt;p&gt;연속값을 갖는 확률변수의 확률분포는 &lt;strong&gt;누적분포함수&lt;/strong&gt;, &lt;strong&gt;확률밀도함수&lt;/strong&gt; 로 표현된다.&lt;/p&gt;

&lt;p&gt;이때 누적분포함수는 다음과 같이 정의된다.&lt;/p&gt;

\[F(x) \equiv P(X &amp;lt; x)\]

&lt;p&gt;확률밀도함수는 누적분포함수의 미분이고, 누적분포함수는 확률변수가 갖는 값 안에서 증가하는 함수다.&lt;/p&gt;

&lt;p&gt;\(X\) 에 대해서 다른 확률변수를 \(Y = g(X)\) 와 같이 정의했을 때, 이 확률변수 \(Y\) 가 어떤 확률분포를 따르게 되는지는 다음과 같다.&lt;/p&gt;

&lt;p&gt;확률변수 \(X\) 의 확률밀도함수를 \(f_X(x)\) 라고 표현한다고 하면,&lt;/p&gt;

\[f_Y(y) = f_X(g(x))|{dx \over dy}|\]

&lt;p&gt;위의 사실 자체보단 위의 식이 어떻게 성립하는지, 다음과 같이 이해하는 것이 좋을듯하다.&lt;/p&gt;

&lt;p&gt;함수 \(g(x)\) 에 의해 \(\Delta x\) 가 \(\Delta y\) 로 매핑된다고 하자. 이때, 확률변수 \(X\) 가 \(\Delta x\) 안의 값을 가질 확률과 확률변수 \(Y\) 가 \(\Delta y\) 안의 값을 가질 확률은 서로 같다. \(\Delta x\) 가 충분히 작다면, \(f_Y(y)\left\vert\Delta y\right\vert = f_X(x)\left\vert\Delta x\right\vert\) 이다.&lt;/p&gt;

&lt;h2 id=&quot;두-실수-확률변수의-결합분포와-조건부분포&quot;&gt;두 실수 확률변수의 결합분포와 조건부분포&lt;/h2&gt;

&lt;p&gt;우선 두 확률변수 \(X, Y\) 의 결합분포는 다음처럼 주어진다.&lt;/p&gt;

\[f_{XY}(x, y)\]

&lt;p&gt;중요한 것은 이것의 부피가 1이라는 점이다.&lt;/p&gt;

&lt;p&gt;조건부 확률과 관련해서 실수하기 쉬운 것은 다음과 같다.&lt;/p&gt;

&lt;p&gt;우선 확률변수 \(X\) 가 a 의 값을 갖는 경우에 \(Y\) 가 갖는 조건부 확률분포를 구한다고 하자. 이 경우 단순히 직관적으로 생각하면, 그 확률밀도함수가 \(f_{XY}(a, y)\) 라고 생각할 수 있다. 하지만 \(f_{XY}(a, y)\) 는 적분을 했을 때, 그 넓이가 1이 되지 않을 수 있다. 그러므로 \(f_{XY}(a, y)\) 에 무언가 상수를 곱해 변형된 함수의 넓이가 1이 되게 해야하며, 그 상수란 \(f_{XY}(a, y)\) 의 넓이의 역수이다. 따라서 조건부 확률밀도함수는 다음과 같이 정의된다. (그리고 그 넓이는 \(f_X(a)\) 이다.)&lt;/p&gt;

\[f_{Y \vert X}(y \vert a) \equiv { f_{XY}(a, y) \over f_{X}(a) }\]

&lt;p&gt;그렇다면 베이즈 공식은 어떻게 되는가?&lt;/p&gt;

&lt;p&gt;\(f_{Y \vert X}\) 과 \(f_X\) 가 주어졌을 때, \(f_{X \vert Y}\) 를 구한다고 하자. \(f_{XY} = f_{Y \vert X} f_X\) 이고, \(f_{Y} = \int_{-\infty}^{\infty} f_{XY} dx\) 이다. 이때, \(f_{XY} = f_{Y \vert X}f_X = f_{X \vert Y}f_Y\) 이므로&lt;/p&gt;

\[f_{X \vert Y} = { f_{Y \vert X}f_X \over {\int_{-\infty}^{\infty} f_{XY} dx}}\]

&lt;p&gt;이다.&lt;/p&gt;

&lt;p&gt;또한 다음이 성립한다.&lt;/p&gt;

\[E[ E[ X \vert Y ] ] = \int_{-\infty}^{\infty} E[ X \vert Y = y ]f_Y(y) dy\]

\[= \int_{-\infty}^{\infty}( \int_{-\infty}^{\infty} x f_{X \vert Y} dx)f_Y(y) dy\]

\[= \int_{-\infty}^{\infty}x( \int_{-\infty}^{\infty} f_{X \vert Y} f_Y(y) dy) dx\]

\[= \int_{-\infty}^{\infty}x( \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy) dx\]

\[= \int_{-\infty}^{\infty}x f_X(x) dx\]

\[= E[ X ]\]

&lt;h2 id=&quot;두-실수-확률변수의-독립성&quot;&gt;두 실수 확률변수의 독립성&lt;/h2&gt;

&lt;p&gt;다음과 같은 경우 두 확률변수 사이에 연관관계가 없다는 사실을 알 수 있다.&lt;/p&gt;

&lt;p&gt;임의의 실수 \(a, b\) 에 대해, \(f_Y(b) = f_{Y \vert X}(b \vert a)\) 이 성립&lt;/p&gt;

&lt;p&gt;위의 식은 다음과 같이 정리가 가능하다.&lt;/p&gt;

\[f_{XY}(a, b) = f_X(a)f_Y(b)\]

&lt;h2 id=&quot;변수-변환&quot;&gt;변수 변환&lt;/h2&gt;

&lt;p&gt;확률변수들 \(X, Y\) 가 모종의 규칙 \(Z = g(X, Y), W = h(X, Y)\) 에 따라 다른 확률변수들로 변환된다고 하자.&lt;/p&gt;

&lt;p&gt;이 경우에 새로운 확률밀도함수는 다음을 만족시킨다.&lt;/p&gt;

\[f_{ZW}(z, w) = { f_{XY}(x, y) \over { \vert Jacobian(g, h) \vert } }\]

&lt;p&gt;Jacobian 은 변수 &lt;strong&gt;변환에 따른 도메인 상에서의 부피확대율&lt;/strong&gt;을 말한다.&lt;/p&gt;

&lt;p&gt;변수변환을 한 이후의 함수가 왜 이런 꼴을 갖는가는 확률밀도함수의 제약인 “도메인상에서의 적분은 1이어야 한다.”를 맞추기 위해, 도메인이 넓어지면 그만큼 함수의 값을 낮춰주고, 도메인이 좁아지면 그만큼 함수의 값을 높여줘야한다는 사실을 기억하면 이해하기 쉽다.&lt;/p&gt;

&lt;h2 id=&quot;실수-확률변수의-기댓값&quot;&gt;실수 확률변수의 기댓값&lt;/h2&gt;

&lt;p&gt;확률변수 \(X\) 에 대한 기댓값은 다음과 같이 정의된다.&lt;/p&gt;

\[E[ X ] = \int_{-\infty}^{\infty} xf(x) dx\]

&lt;p&gt;왜 위와 같이 정의되는가는 적분을 리만합의 형태로 표현하고 나면 이해하기 쉽다. 요컨대 \(f(x)dx\) 가 \(X\) 가 \(dx\) 안의 값을 가질 확률을 의미한다고 해석하면, 확률변수가 이산적인 경우와 차이가 없다.&lt;/p&gt;

&lt;p&gt;이산적인 경우와 마찬가지로 다음의 연산들이 성립한다.&lt;/p&gt;

\[E[ X + Y ] = E[ X ] + E[ Y ]\]

\[E[ X - Y ] = E[ X ] + E[ Y ]\]

\[E[ X + c ] = E[ X ] + c\]

\[E[ cX ] = cE[ X ]\]

\[E[ X / c ] = E[ X ] / c, 단 c \neq 0\]

&lt;p&gt;또한 두 확률 변수 \(X, Y\) 가 &lt;strong&gt;서로 독립&lt;/strong&gt;인 경우에만&lt;/p&gt;

\[E[ XY ] = E[ X ]E[ Y ]\]

&lt;h2 id=&quot;중심극한정리&quot;&gt;중심극한정리&lt;/h2&gt;

&lt;p&gt;어떤 사건이 계속해서 독립적으로 발생하고, 발생할 때 마다 그 확률분포는 일정하다고 하자. 만약 그 사건이 발생함에 따라서 그 영향이 누적되면, 누적된 영향은 그 자체로도 확률변수이다. 이때 이 확률변수는 어떤 확률분포는 정규분포 형태를 따른다.&lt;/p&gt;

\[Y \equiv X_1 + X_2 + \cdots + X_n\]

&lt;p&gt;위와 같이 정의하면, \(Y\) 의 확률분포는 점차 좌우로 퍼지면서 Variance 가 증가한다. 정규분포 형태인지 확인하기 위해서 normalize 해주기 위해 다음과 같이 새로이 정의한다.&lt;/p&gt;

\[Y \equiv { { X_1 + X_2 + \cdots + X_n } \over { \sqrt{n}\sigma } }\]

&lt;p&gt;밑의 \(\sqrt{n}\sigma^2\) 는 \(X_1 + X_2 + \cdots + X_n\) 의 표준편차이다.&lt;/p&gt;

\[V[ X_1 + X_2 + \cdots + X_n ] = V[ X_1 ] + V[ X_2 ] + \cdots + V[ X_n ]\]

\[= \sigma^2 + \sigma^2 + \cdots + \sigma^2 = n\sigma^2\]
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note7</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note7</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>(확률과 통계) 3. 이산값의 확률분포</title>
        <description>&lt;p&gt;이산값의 확률분포라고 했는데, 이산값이라는 것은 일단 어떤 확률변수가 가질 수 있는 값이 이산적인 경우를 말한다. 책에서는 정수를 갖는 확률변수를 다루었다.&lt;/p&gt;

&lt;h2 id=&quot;이항분포&quot;&gt;이항분포&lt;/h2&gt;

&lt;p&gt;이항분포는 확률분포의 일종으로, 확률 변수 X 를 다음과 같이 정의한다고 했을 때,&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“앞면이 나올 확률이 p, 뒷면이 나올 확률이 q 즉 (1 - p) 일 때, 동전을 n 번 던졌을 때 앞면이 나온 횟수”&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;확률 변수 X 가 갖는 확률분포를 말한다.&lt;/p&gt;

\[P_{n,p}(X = k) = {_{n}\mathrm{C}_{k}p^{k}(1-p)^{n-k} }\]

&lt;h2 id=&quot;기댓값&quot;&gt;기댓값&lt;/h2&gt;

&lt;p&gt;어떤 확률변수가 특정확률분포를 따른다고 했을 때, 그 확률변수가 평균적으로 취할 값이 무엇인가는 다음과 같이 계산된다.&lt;/p&gt;

\[E[ X ] = \sum_{k} k P(X = k)\]

&lt;p&gt;정의 자체가 Series 로 표현되기 때문에, 만약 infinite series (확률 변수가 가질 수 있는 값이 무한하게 많은 경우) 가 수렴하지 않는 경우에 기대값이 존재하지 않을 수도 있다.&lt;/p&gt;

&lt;p&gt;위와 같이 생각해보면 기댓값은 확률변수의 집합을 도메인으로 삼는 어떤 함수라고 생각할수도 있지 않을까 싶다.&lt;/p&gt;

&lt;h3 id=&quot;기댓값-연산&quot;&gt;기댓값 연산&lt;/h3&gt;

\[E[ X + Y ] = E[ X ] + E[ Y ]\]

\[E[ X - Y ] = E[ X ] + E[ Y ]\]

\[E[ X + c ] = E[ X ] + c\]

\[E[ cX ] = cE[ X ]\]

\[E[ X / c ] = E[ X ] / c, 단 c \neq 0\]

\[E[ X^2 ] = E[ X ]^2 + V[ X ]\]

\[E[ (X - a)^2 ] = (\mu - a)^2 + \sigma^2 \cdots (1)\]

&lt;p&gt;또한 두 확률 변수 \(X, Y\) 가 &lt;strong&gt;서로 독립&lt;/strong&gt;인 경우에만&lt;/p&gt;

\[E[ XY ] = E[ X ]E[ Y ]\]

&lt;p&gt;위에서 \((1)\) 은 상당히 재미있는 해석이 되는데, 어떤 상수 \(a\) 와 확률변수 \(X\) 의 제곱오차 평균은 그들의 &lt;strong&gt;편향(Bias)&lt;/strong&gt; \((\mu - a)^2\) 와 확률변수 고유의 &lt;strong&gt;편차&lt;/strong&gt; \(\sigma^2\) 의 합으로 생각할 수 있다는 것이다. (즉, 두 종류의 오차로 쪼개져서 표현 가능해진다.)&lt;/p&gt;

&lt;h2 id=&quot;분산-및-표준편차&quot;&gt;분산 및 표준편차&lt;/h2&gt;

&lt;p&gt;확률변수는 확률변수이기 때문에 기댓값에서 벗어나는 값 또한 갖는다. 다음 관심사는 이 벗어난 값이 기댓값에서 얼마나 벗어냐느냐를 어떻게 수치화하는가? 이다. 기댓값과의 오차의 제곱의 평균을 분산이라고 정의하고, 그 척도의 일종으로 쓴다.&lt;/p&gt;

\[V[ X ] = E[ (X - \mu )^2 ], \mu = E[ X ]\]

&lt;p&gt;그리고 이것에 제곱근을 씌워, X 와 단위를 같게 한 것이 표준편차이다.&lt;/p&gt;

\[\sigma = \sqrt{V[ X ]}\]

&lt;h2 id=&quot;분산-연산&quot;&gt;분산 연산&lt;/h2&gt;

\[V[ X + c ] = V[ X ]\]

\[V[ cX ] = c^2 V[ X ]\]

&lt;p&gt;만약 확률변수 \(X, Y\) 가 &lt;strong&gt;서로 독립&lt;/strong&gt;이라면,&lt;/p&gt;

\[V[ X + Y ] = V[ X ] + V[ Y ]\]

&lt;h2 id=&quot;큰-수의-법칙&quot;&gt;큰 수의 법칙&lt;/h2&gt;

&lt;p&gt;큰 수의 법칙이라는 것은 어떤 확률변수의 값을 측정하는 실험을 수없이 많이 반복한 이후, 그 모든 실험에서 확률변수가 취한 값들의 평균값을 구했을 때, 그 값이 확률변수의 기댓값과 같다. 라는 것을 의미한다.&lt;/p&gt;

&lt;p&gt;단 기댓값이 존재하지 않는 경우도 있으니, 조심해야한다.&lt;/p&gt;

&lt;h3 id=&quot;조건부-기댓값&quot;&gt;조건부 기댓값&lt;/h3&gt;

&lt;p&gt;조건부 기댓값은 다음과 같이 정의된다.&lt;/p&gt;

\[E[ Y | X = a ] = \sum_{b}b P(Y = b | X = a)\]

&lt;p&gt;그냥 간단히, 어떤 확률변수 \(X\) 가 결정된 상황에서 또 다른 확률변수 \(Y\) 는 어떤 기댓값을 갖는가? 를 구하는 것이다.&lt;/p&gt;

&lt;p&gt;이때 확률변수 \(X\) 의 값에 따라서 \(E[ Y ]\) 또한 변화한다는 점에 유의해야한다.&lt;/p&gt;

&lt;p&gt;즉 \(E[ Y \vert X = a ]\) 자체도 확률변수가 되며, 이를 \(E[ E[ Y \vert X ] ]\) 라고 표현한다고 하자. 이때 이것은 다음을 만족한다.&lt;/p&gt;

\[E[ E[ Y \vert X ] ] = E[ Y ]\]
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note6</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note6</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>강화학습 1</title>
        <description>&lt;h2 id=&quot;reinforcement-learning-과-supervised-learning-의-차이&quot;&gt;Reinforcement Learning 과 Supervised Learning 의 차이&lt;/h2&gt;

&lt;p&gt;Reinforcement Learning (RL) 과 Supervised Learning 의 차이점은 RL 의 경우에는 지도 데이터가 없다는 점이다. Agent 가 스스로 어떤 결정을 내리고, 그것에 대해서 Reward 만을 받는 것으로 학습해 나간다.&lt;/p&gt;

&lt;p&gt;만약 목적함수 F 를 정의하고, 이것을 곧 Reward 로 세팅하게되면, 목적함수 F 를 Maximize 하는 액션들을 스스로 학습한다는 점에서, Optimization Problemt Solving 의 한 방법이라고 볼 수 있지 않을까 싶다.&lt;/p&gt;

&lt;h2 id=&quot;용어-정리&quot;&gt;용어 정리&lt;/h2&gt;

&lt;h3 id=&quot;reward&quot;&gt;Reward&lt;/h3&gt;

&lt;p&gt;“scalar feedback signal”&lt;/p&gt;

&lt;p&gt;t step 에 얻은 reward 를 \(R_t\) 라고 하면,&lt;/p&gt;

&lt;p&gt;Agent의 목적은 이렇게 얻어낸 reward 들의 총합을 최대화하는 것을 목적으로 한다.&lt;/p&gt;

&lt;h3 id=&quot;reward-hypothesis&quot;&gt;Reward Hypothesis&lt;/h3&gt;

&lt;p&gt;“모든 목적은 cumulative reward 를 극대화하는 것으로 표현될 수 있다.”&lt;/p&gt;

&lt;p&gt;요컨대 reward 가 어떤 것을 기준으로 계산되어 Agent 에게 알려지는가가 중요하다.&lt;/p&gt;

&lt;h3 id=&quot;sequential-decision-making&quot;&gt;Sequential Decision Making&lt;/h3&gt;

&lt;p&gt;RL은 미래의 Reward 를 최대화하는 행동들을 결정한다. Reward 가 delayed 될 수도 있다. 요컨대 Long-Term reward 를 극대화하는 행동들을 학습한다.&lt;/p&gt;

&lt;h3 id=&quot;agent&quot;&gt;Agent&lt;/h3&gt;

&lt;p&gt;뇌와 같은 것(학습자)&lt;/p&gt;

&lt;h3 id=&quot;environment&quot;&gt;Environment&lt;/h3&gt;

&lt;p&gt;뇌 외의 모든 것&lt;/p&gt;

&lt;h3 id=&quot;action&quot;&gt;Action&lt;/h3&gt;

&lt;p&gt;행동의 단위.&lt;/p&gt;

&lt;p&gt;Environment 는 Action을 받고, Reward 와 Observation(변화한 상황, 세상의 현재 모습을 의미?) 을 Agent 에게 알린다.
Agent는 Observation 과 Reward 를 받고 다음 Action 을 결정한다.&lt;/p&gt;

&lt;h3 id=&quot;history&quot;&gt;History&lt;/h3&gt;

&lt;p&gt;observation, action, reward 의 나열(기록을 한다.)&lt;/p&gt;

\[H_t = O_1, R_1, A_1, ... , A_{t-1}, O_t, R_t\]

&lt;p&gt;Agent 는 History 로부터 Action 을 정한다. Environment 는 History 로부터 Reward 와 Observation 을 정한다.&lt;/p&gt;

&lt;h3 id=&quot;state&quot;&gt;State&lt;/h3&gt;

&lt;p&gt;다음에 무슨일이 일어날지 결정할 때 쓰이는 정보. History 로부터 정제된 정보로, 그냥 평범하게 History 에서 무언가 Decision Making 을 하기 편리하게 요리조리 계산한 데이터 더미라고 보면 될 듯 하다.&lt;/p&gt;

\[S_t = f(H_t)\]

&lt;p&gt;Envrionment State \(S_t^e\) 는 envirionment 가 다음 observation 과 reward 를 결정하는데 쓰이는 모든 정보들(보통 private)을 의미한다.&lt;/p&gt;

&lt;p&gt;Agent State \(S_t^a\) 는 다음 action 을 결정할 때 쓰이는 정보들이다. 따라서 state 를 어떻게 정의하느냐(3 step 동안 종이 울린 횟수 등)에 따라서 판단하는 방식이 바뀐다.&lt;/p&gt;

&lt;h3 id=&quot;어떤-state-가-markov-하다&quot;&gt;어떤 State 가 Markov 하다.&lt;/h3&gt;

&lt;p&gt;“새로운 state 가 바로 이전 step 의 state 에만 의존하여 결정된다.”&lt;/p&gt;

&lt;p&gt;요컨대 과거와는 독립적이고, 현재만이 중요하다.&lt;/p&gt;

&lt;p&gt;자동차를 운전한다고 했을 때, 어떤 목적지에 다다르기 위한 의사결정을 한다고 하자. 주변의 모든 사물과 차의 현재 위치, 속도 등을 알면 이전에 내가 어떻게 운전을 해왔는지는 중요하지 않고, 현재의 상태만 있으면 충분하다. 하지만 자동차의 속도 정보를 모른다고 하면, 이후의 state (내가 페달을 더 밟든 말든)는 현재 state 와 이전 history (내가 브레이크를 어느정도 밟아왔고, 페달은 어떻게 밟았는지 등)에 의존하게 된다.&lt;/p&gt;

&lt;p&gt;그런 의미에서 information state 혹은 Markov state 는 history 로부터 모든 유용한 정보를 전부 담은 state 를 말한다. state 는 Markov 해야 한다.&lt;/p&gt;

&lt;h3 id=&quot;fully-observable-environment&quot;&gt;Fully Observable Environment&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;full observability&lt;/strong&gt; 는 agent 가 environment state 를 바로 볼 수 있음을 말한다.&lt;/p&gt;

\[O_t = S_t^a = S_t^e\]

&lt;p&gt;이 경우를 Markov decision process 라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;partial observability&lt;/strong&gt; 는 agent 가 environment 를 간접적으로 관찰함을 말한다. (로봇의 카메라가 현재의 위치를 안 알려준다거나, 포커 플레이어 Agent 가 공개된 카드만을 본다거나)&lt;/p&gt;

&lt;p&gt;이 경우를 Partially observable Markov decision process 라고 한다.&lt;/p&gt;

&lt;p&gt;agent 는 이 경우 자기만의 독자적인 state 표현을 구성해야 한다.&lt;/p&gt;

&lt;h2 id=&quot;agent-의-구성-요소&quot;&gt;Agent 의 구성 요소&lt;/h2&gt;

&lt;p&gt;agent 는 policy, value function, model 의 구성 요소를 가질 수 있다. 하지만 3개 전부 다 가질 필요는 없다.&lt;/p&gt;

&lt;h3 id=&quot;policy&quot;&gt;policy&lt;/h3&gt;

&lt;p&gt;agent 의 행동을 규정하는 것. state 를 넣으면 action 을 반환한다.&lt;/p&gt;

&lt;p&gt;deterministic policy 는 state 하나에 하나의 action 을 매핑한다.&lt;/p&gt;

\[a = \pi (s)\]

&lt;p&gt;stochastic policy 는 state 하나에 여러 액션의 확률을 준다.&lt;/p&gt;

\[\pi (a | s)\]

&lt;h3 id=&quot;value-function&quot;&gt;value function&lt;/h3&gt;

&lt;p&gt;이후 future reward 의 총 합산을 예측해준다. 현재 state 가 좋은지 안 좋은지 등을 체크할 때도 쓰일 수 있다.&lt;/p&gt;

\[v_\pi (s) = E_\pi [ R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s]\]

&lt;p&gt;위의 의미는 현재 상태가 \(s\) 이고, agent 가 policy \(\pi\) 를 따른다면, 미래에 얻을 총 Reward 의 기대값을 말한다. (polcy 가 deterministic 하다 할지라도, environment 가 확률적 요소를 가질 수 있기에, 쉽게 기대값을 제거할 수 있다거나 할 순 없다.)&lt;/p&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;환경이 어떻게될 지 예측하는 것.&lt;/p&gt;

&lt;p&gt;action 을 수행했을 때, next state 를 예측하는 것과 next reward 를 예측하는 것이 있다.&lt;/p&gt;

\[P_{ss'}^a = P[S_{t+1} = s' | S_t = s, A_t = a]\]

&lt;p&gt;위의 식에서 \(P_{ss'}^a\) 는 \(s\) 에서 \(a\) 를 했을 때, \(s'\) 가 될 확률을 의미한다.&lt;/p&gt;

\[R_s^a = E[R_{t+1} | S_t = s, A_t = a]\]

&lt;h2 id=&quot;agent-의-분류-방법&quot;&gt;Agent 의 분류 방법&lt;/h2&gt;

&lt;h3 id=&quot;value-based&quot;&gt;Value Based&lt;/h3&gt;

&lt;p&gt;value function 만을 갖고 있다.&lt;/p&gt;

&lt;h3 id=&quot;policy-based&quot;&gt;Policy Based&lt;/h3&gt;

&lt;p&gt;policy 만을 갖고 있다.&lt;/p&gt;

&lt;h3 id=&quot;actor-critic&quot;&gt;Actor Critic&lt;/h3&gt;

&lt;p&gt;policy 와 value function 을 둘 다 갖고 있다.&lt;/p&gt;

&lt;h3 id=&quot;model-free&quot;&gt;Model Free&lt;/h3&gt;

&lt;p&gt;model 을 갖고 있지 않다.&lt;/p&gt;

&lt;h3 id=&quot;model-based&quot;&gt;Model Based&lt;/h3&gt;

&lt;p&gt;model 을 갖고 있다.&lt;/p&gt;

&lt;h2 id=&quot;문제의-분류&quot;&gt;문제의 분류&lt;/h2&gt;

&lt;h3 id=&quot;learning&quot;&gt;Learning&lt;/h3&gt;

&lt;p&gt;environment 가 처음에 알려지지 않는다. 하지만 environment 와 상호작용을 하면서 policy 를 개선해나간다.&lt;/p&gt;

&lt;p&gt;예를 들어, 게임의 규칙을 모르고 이것 저것 조작해보면서 score 를 따는 문제는 learning 이다.&lt;/p&gt;

&lt;h3 id=&quot;planning&quot;&gt;Planning&lt;/h3&gt;

&lt;p&gt;environment 의 모델이 알려져 있다. (reward 가 어떻게 되는지 알고, state transition 을 안다.) 그래서 agent 는 environment 와의 상호작용을 안해도, environment 의 모델을 통해서 여러 상태를 탐색하는 것이 가능하다. 이를 통해 policy 를 개선해나간다.&lt;/p&gt;

&lt;p&gt;이 경우에는 model 에게 이렇게 하면 어떤 상태가 되는가? 어떤 점수를 얻는가? 를 계속해서 물어(Query)볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;exploration-과-exploitation&quot;&gt;Exploration 과 Exploitation&lt;/h2&gt;

&lt;h3 id=&quot;exploration&quot;&gt;Exploration&lt;/h3&gt;

&lt;p&gt;environment 에 대한 정보를 찾는다.&lt;/p&gt;

&lt;h3 id=&quot;exploitation&quot;&gt;Exploitation&lt;/h3&gt;

&lt;p&gt;알려진 정보로부터 reward 를 극대화한다.&lt;/p&gt;

&lt;p&gt;예를 들면 자신이 아는 가장 좋은 음식점을 가는 것은 exploitation 이고, 자신이 모르는 새로운 음식점을 가는 것은 exploration 이다.&lt;/p&gt;

&lt;h2 id=&quot;prediction-과-control&quot;&gt;Prediction 과 Control&lt;/h2&gt;

&lt;h3 id=&quot;prediction&quot;&gt;Prediction&lt;/h3&gt;

&lt;p&gt;미래를 평가하는 것 (value function 을 학습시키는 문제)&lt;/p&gt;

&lt;h3 id=&quot;control&quot;&gt;Control&lt;/h3&gt;

&lt;p&gt;미래를 최적화하는 것 (policy 를 찾는 문제)&lt;/p&gt;
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note5</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note5</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>(확률과 통계) 2. 결합 확률, 조건부 확률, 독립성</title>
        <description>\[P(A, B)\]

\[P(A | B)\]

\[P(A, B) = P(A)P(B)\]

&lt;p&gt;2장에서는 확률 변수 사이에 결합 확률과 조건부 확률, 독립성에 대해서 다룬다.&lt;/p&gt;

&lt;p&gt;확률 변수는 그 자체로 확률 공간 \(\Omega\) 상에서 서로 독자적인 확률 분포를 갖는다. 예를 들면&lt;/p&gt;

\[P(A = 0) = 0.1, P(A = 1) = 0.5, P(A = 2) = 0.4\]

&lt;p&gt;위와 같다.&lt;/p&gt;

&lt;p&gt;그런데 확률 변수들은 확률 공간을 Domain 으로 하는 함수들이라는 점에서, 서로 다른 두 확률 변수가 주어졌을 때, &lt;strong&gt;이 두 변수가 각자 특정 값을 취하는데 있어서 서로 상관관계, 혹은 연관성(Inclination ?)가 있는가?&lt;/strong&gt; 라는 질문을 할 수 있다.&lt;/p&gt;

&lt;p&gt;예를 들면 이런 것이다.&lt;/p&gt;

&lt;p&gt;“A 변수가 2의 값을 가질 때면, B 변수가 5의 값을 갖는 경우가 많더라.”&lt;/p&gt;

&lt;p&gt;재미있는 점은 각각의 확률 변수들의 개개의 확률 분포를 파악하는 것 만으로는 그들 사이에 연관 관계를 알기에 부족하고, 결합 확률이나 조건부 확률 등을 알아야 그 변수들 사이에 연관성을 파악하는 것이 가능하다는 점이다.&lt;/p&gt;

&lt;p&gt;이러한 측면에서, 두 확률 변수 사이에 의존관계가 있는지 없는지(즉 독립성 여부 판단)를 파악하고,
\(P(A|B)\) 와 같은 조건부 확률을 알았을 때,
\(P(B|A)\) 와 같이 두 변수 사이에 생각할 수 있는 다른 확률을 계산하는 법을 익히는 것이 2장의 목적이었다.&lt;/p&gt;

&lt;p&gt;참고로, 다변수 상황에서의 확률 변수 독립성 판단은 다음과 같이 한다.&lt;/p&gt;

\[임의의 a, b, c, d 에 대해\]

\[P(A = a, B = b, C = c, D = d) = P(A = a)P(B = b)P(C = c)P(D = d)\]
</description>
        <pubDate>Mon, 07 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note4</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note4</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>VSCode 관련 이모저모</title>
        <description>&lt;p&gt;VSCode 를 리눅스에서 사용할 편집기로 결정했다. 그런데 이거 좀 만만하지가 않다. VS에서 자주 쓰던 기능이나 숏컷들이 전부 다른 형태가 되었다. 그래서 VSCode 를 사용하면서 익힌 VSCode 관련 지식들을 이곳에 메모하고자 한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;자주-쓰이는-숏컷&quot;&gt;자주 쓰이는 숏컷&lt;/h2&gt;

&lt;p&gt;ctrl + p : 파일 이름으로 파일을 찾고 싶을 때, 요긴하게 쓰인다.&lt;/p&gt;

&lt;p&gt;ctrl + shift + I : 포맷팅 (개인적으로 자동으로 해주는게 좋았는데, 별도의 세팅 없이는 수동으로 해주어야 하는 모양이다.)&lt;/p&gt;

&lt;p&gt;ctrl + ` : 터미널을 열어준다. cmake 나 make, jekyll 커맨드 등을 실행할 때 편리하다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;좋은-extension-들&quot;&gt;좋은 Extension 들&lt;/h2&gt;

&lt;p&gt;Lua Helper (yinfei) : 상당히 괜찮다. Formatting 과 Intellisense 등이 지원된다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;include-경로-설정&quot;&gt;Include 경로 설정&lt;/h2&gt;

&lt;p&gt;교수님의 프로젝트를 열면, 모든 Include 밑에 빨간 줄이 그어지며 해당 헤더파일을 찾을 수 없다는 오류가 출력된다.&lt;/p&gt;

&lt;p&gt;요컨대 포함 경로를 세팅하라는 말이다.&lt;/p&gt;

&lt;p&gt;VSCode 에서는 C/C++ Extension 이라는 것을 설치해서 c++ 프로젝트를 구성하게 되는데, 이 C/C++ Extension 의 세팅에서 포함경로를 세팅해줄 필요가 있다.&lt;/p&gt;

&lt;p&gt;이는 c_cpp_properties.json 파일에 저장되는데,&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;configurations&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;Linux&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;includePath&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;${workspaceFolder}/**&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/ois&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/libxml2&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/OGRE&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/OIS&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/lua5.1&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/eigen3&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/python2.7&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/include/python3.6&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;defines&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
            &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;compilerPath&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/usr/bin/gcc&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;cStandard&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;gnu11&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;cppStandard&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;gnu++14&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;intelliSenseMode&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;gcc-x64&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;위와 같이 includePath 라는 프로퍼티에 사용하고자 하는 경로를 채워넣으면 된다.&lt;/p&gt;

&lt;p&gt;나는 CMake 파일을 직접 읽어서 add_include_directory 함수 밑의 경로를 전부 긁어다 넣었는데, 분명 더 쉬운 방법이 있지 않을까…&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;python-가상환경구축&quot;&gt;Python 가상환경구축&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#!/bin/bash

virtualenv --system-site-packages -p python3 ./venv
source ./venv/bin/activate
pip install --upgrade pip
deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위와 같은 배시 스크립트를 가상환경을 구축하고자 하는 폴더에 넣고 실행한다.
이후, VSCode 에서 해당 폴더를 열고 Ctrl + Shift + P =&amp;gt; Python: Select Interpreter =&amp;gt; 방금 만든 가상환경 설정&lt;/p&gt;

&lt;p&gt;VSCode 에서는 가상환경 구축이 안되면, 파이썬을 작성할 때 여러가지 하자가 있더라..&lt;/p&gt;
</description>
        <pubDate>Sun, 06 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note3</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note3</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>CMake 튜토리얼 관련 자료 모음</title>
        <description>&lt;p&gt;CMake 을 사용할 일이 생겼는데, 기본적인 것을 익히기에 좋은 사이트들의 목록을 정리하고자 한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.tuwlab.com/27234&quot;&gt;우선 쉬운 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gist.github.com/luncliff/6e2d4eb7ca29a0afd5b592f72b80cb5c&quot;&gt;좀 더 전문적인? 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cgold.readthedocs.io/en/latest/overview.html&quot;&gt;위의 블로그에서 알려준 좋은 튜토리얼&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;cmake_module_path--cmake-모듈-찾는-위치&quot;&gt;CMAKE_MODULE_PATH : .cmake 모듈 찾는 위치&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;set(CMAKE_MODULE_PATH 경로1 경로2 ...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위와 같이 선언해두면, 이후에 include 를 통해서 cmake 모듈 (아마 .cmake 확장자를 가진 친구들)을 찾아 올 때, 해당 경로에서 우선적으로 찾아온다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;include_directories-와-target_include_directories--헤더파일-폴더&quot;&gt;include_directories 와 target_include_directories : 헤더파일 폴더&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;include_directories(x/y)
target_include_directories(t x/y)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 함수 둘 다 헤더 파일을 찾을 폴더를 명시하는 것인데,
무슨 차이인가 궁금하여 검색해보았다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;include_directories(x/y)&lt;/strong&gt; affects directory scope. All targets in this CMakeList, as well as those in all subdirectories added after the point of its call, will have the path x/y added to their include path.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;target_include_directories(t x/y)&lt;/strong&gt; has target scope—it adds x/y to the include path for target t.&lt;/p&gt;

&lt;p&gt;You want the former one if all of your targets use the include directories in question. You want the latter one if the path is specific to a target, or if you want finer control of the path’s visibility. The latter comes from the fact that target_include_directories() supports the PRIVATE, PUBLIC, and INTERFACE qualifiers&lt;/p&gt;

&lt;p&gt;요컨대 전역과 지역의 차이(?)인 듯.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;link_directories--링크-시-라이브러리를-찾을-경로&quot;&gt;link_directories : 링크 시 라이브러리를 찾을 경로&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;link_directories(&amp;lt;경로&amp;gt; &amp;lt;경로&amp;gt; ...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;위 함수는 링크를 진행할 때, 라이브러리를 찾을 경로를 명시한다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;add_subdirectory--다른-cmakeliststxt-실행&quot;&gt;add_subdirectory : 다른 CMakeLists.txt 실행&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;add_subdirectory(경로 [바이너리 경로])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위 함수는 해당 경로에서 CMakeLists.txt 를 찾아 실행하는 역할을 한다. 솔직히 아직까진 왜 CMakeLists.txt 와 .cmake 로 구분되는 CMake 모듈을 분리해놓았는지 잘 모르겠다. 분명 관리나 공유? 의 측면에서 이점이 있으니 그렇게 한 것이 아닐까.&lt;/p&gt;

&lt;p&gt;그리고 바이너리 경로라는 것을 지정할 수 있다. add_subdirectory 의 호출로 만들어진 결과물들을 저장하는 경로를 지정하는 것이다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;project-name_source_dir--자동으로-생성되는-변수도-있다&quot;&gt;project name_SOURCE_DIR : 자동으로 생성되는 변수도 있다.&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;include_directories(
${MainLib_SOURCE_DIR}/../../dependencies/OgreSDK_vc10_v1-8-1/boost
${MainLib_SOURCE_DIR}/../../dependencies/OgreSDK_vc10_v1-8-1/include/OGRE
${MainLib_SOURCE_DIR}/../../dependencies/OgreSDK_vc10_v1-8-1/include/OIS
${MainLib_SOURCE_DIR}/../../dependencies/fltk-1.1.10
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;처음에 위의 코드를 보고 벙쪘다. 왜냐하면 MainLib_SOURCE_DIR 라는 변수는 그 어떤 파일에서도 set 해주지 않았기 때문이다. 하지만 CMake 에서&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;project name&amp;gt;_SOURCE_DIR
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;라는 형식으로 세팅된 project 이름에 대해서 자동으로 변수를 생성해준다는 것을 알 수 있었다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;target_link_libraries-와-target_link_directories&quot;&gt;target_link_libraries 와 target_link_directories&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;target_link_libraries(&amp;lt;타겟&amp;gt; 라이브러리경로1 ...)
target_link_directories(&amp;lt;타켓&amp;gt; 라이브러리를 찾을 경로1 ...)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;타겟&gt;으로 명시되는 (무조건 add_exexutable() 혹은 add_library() 로 만들어진 것) 것을 구성할 때, 라이브러리를 찾는 경로나 라이브러리 자체를 명시하는 함수이다.

---
## .a vs .so

프로그래머 실격이다. 이것이 무엇인지 모르겠더라.

.a 는 정적 라이브러리

.so 는 동적 라이브러리라고 한다.

사용하는 운영체제가 ubuntu 인데, 윈도우와는 라이브러리 확장자가 다른 모양??
&lt;/타겟&gt;
</description>
        <pubDate>Fri, 04 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note2</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note2</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>(확률과 통계) 1. 확률 변수에 대해서 익힌 것</title>
        <description>&lt;p&gt;이번 학기에 인공지능 과목을 듣는데,&lt;/p&gt;

&lt;p&gt;교수님이 확률 통계에 대해서 모르면, F 를 받을 수 있다고 겁을 주었다.&lt;/p&gt;

&lt;p&gt;나는 개인적으로 Calculus 와 Linear Algebra 에는 자신이 있었는데,&lt;/p&gt;

&lt;p&gt;확률 통계에 대해서는 완전히 문외한이기 때문에 새롭게 익히기로 했다.&lt;/p&gt;

&lt;p&gt;그리하여 원서를 읽기에는 시간이 오래 걸릴테니,&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791160507829&amp;amp;orderClick=LEa&amp;amp;Kc=&quot;&gt;&lt;em&gt;프로그래머를 위한 확률과 통계&lt;/em&gt;&lt;/a&gt;라는 책을 읽기로 했다.&lt;/p&gt;

&lt;p&gt;조금씩 읽으면서, 생각과는 달랐거나 기억할만한 내용들이 있다면 이곳에 노트를 할 것이다.&lt;/p&gt;

&lt;h2 id=&quot;노트-본문&quot;&gt;노트 본문&lt;/h2&gt;

&lt;p&gt;확률 변수에 대해서 새로운 시각을 얻었다.&lt;/p&gt;

&lt;p&gt;확률 변수라는 것은 어떤 값인데 특정확률로 어떤 값을 갖는 값이라고만 생각하고 있었다.&lt;/p&gt;

&lt;p&gt;하지만 확률 변수란 확률 공간이라는 곳에서 정의된 &lt;strong&gt;함수&lt;/strong&gt;이며,&lt;/p&gt;

&lt;p&gt;이 확률 변수가 어떤 값을 가질 확률이라는 것은 다음과 같다.&lt;/p&gt;

\[P(X = x) = P(Y)\]

&lt;p&gt;여기서 \(Y\) 라는 것은&lt;/p&gt;

\[Y = \left\{ y \in \Omega | X(y) = x \right\}\]

&lt;p&gt;위와 같이 정의된다. 즉 확률 공간 \(\Omega\) 의 부분집합이다.&lt;/p&gt;

&lt;p&gt;책에서는 &lt;em&gt;이 확률 공간이라는 것이 어떻게 생겼는가?&lt;/em&gt; 보단,&lt;/p&gt;

&lt;p&gt;그 위에서 정의되는 확률 변수와 확률 함수 \(p\) 에 초점을 맞춰야한다고 했다.&lt;/p&gt;

&lt;p&gt;우선 어떤 시스템이 가질 수 있는 모든 상태를 확률 공간이라고 생각하고,&lt;/p&gt;

&lt;p&gt;확률 공간의 부분집합 \(X\)를 어떤 값에 매핑시키는 확률 함수 \(P\)가&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;알맞게(아마 공리에 맞게)&lt;/strong&gt; 정의되면, 확률 공간에서 정의되는 확률 변수에 대해&lt;/p&gt;

&lt;p&gt;그 확률 변수가 어떤 값을 지니는가를 이 확률 함수를 통해 계산하는 것이 가능하다.&lt;/p&gt;

&lt;p&gt;즉, 시스템이 어떤 상태를 가질 확률을 구하는 것이 가능하다.&lt;/p&gt;
</description>
        <pubDate>Fri, 04 Sep 2020 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2020-09/note1</link>
        <guid isPermaLink="true">http://localhost:4000/2020-09/note1</guid>
        
        
        <category>study</category>
        
      </item>
    
      <item>
        <title>Sample Post</title>
        <description>&lt;p&gt;This is a sample post.&lt;br /&gt;
&lt;dfn info=&quot;You can add extra information&quot;&gt;Hover&lt;/dfn&gt; me.&lt;/p&gt;

&lt;h2 id=&quot;header&quot;&gt;Header&lt;/h2&gt;

&lt;h1 id=&quot;head-1-&quot;&gt;Head 1: ‘#’&lt;/h1&gt;
&lt;h2 id=&quot;head-2-&quot;&gt;Head 2: ‘##’&lt;/h2&gt;
&lt;h3 id=&quot;head-3-&quot;&gt;Head 3: ‘###’&lt;/h3&gt;

&lt;h2 id=&quot;code-block&quot;&gt;Code block&lt;/h2&gt;
&lt;p&gt;A Python Example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;quote&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# Socrates
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'The only true wisdom is in knowing you know nothing.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;A HTML Example:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;&amp;lt;!DOCTYPE html&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;html&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;lang=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;head&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;meta&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;charset=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;title&amp;gt;&lt;/span&gt;quote&lt;span class=&quot;nt&quot;&gt;&amp;lt;/title&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/head&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;body&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- Maya Angelou --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;p&amp;gt;&lt;/span&gt;Try to be a rainbow in someone's cloud.&lt;span class=&quot;nt&quot;&gt;&amp;lt;/p&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/body&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/html&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A C Example:&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#include &amp;lt;stdio.h&amp;gt;
&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// Confucius&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;printf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Everything has beauty, but not everyone sees it.&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 01 Nov 2017 00:00:00 +0900</pubDate>
        <link>http://localhost:4000/2017-11/s2</link>
        <guid isPermaLink="true">http://localhost:4000/2017-11/s2</guid>
        
        
        <category>writing</category>
        
      </item>
    
  </channel>
</rss>