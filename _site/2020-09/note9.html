<!DOCTYPE html>
<html>



<head>
  <title>강화학습 2 | JunWoo's Blog</title>
  
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="author" content="JunWoo Lee">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="/assets/css/main.css" media="all">
  <link rel="canonical" href="http://localhost:4000/2020-09/note9">
  <link rel="alternate" type="application/rss+xml" title="JunWoo&#39;s Blog"
    href="/feed.xml" />

  <!-- favicon -->
  <link rel="shortcut icon" href="/assets/img/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="/assets/img/favicon.ico" type="image/x-icon" />

  <!-- font -->
  <link href="https://fonts.googleapis.com/css?family=Titillium+Web:400,400i" rel="stylesheet">
</head>

<body>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <div class="content">

    <header class="header">

  <div class="header_content">
    <label class="theme_changer">
      <input theme_toggle type="checkbox">
      <div class="button"></div>
    </label>
    <a class="header_circle" href="/">
      <img src="/assets/img/cat.jpg" alt="catbook">
    </a>
    <span class="header_name">JunWoo Lee</span>
    <span class="header_job">programmer</span>
    <span class="header_mes"></span>

    <nav class="nav">
      
      <ul class="nav_list">
        <li class="nav_item">
          <a href="/aboutme.html">about</a>
        </li>
      </ul>
      
      
      
      <ul class="nav_list">
        <li class="nav_item">
          <a href="/categories/writing" id="aa">writing
            (1)</a>
        </li>
      </ul>
      
      
      <ul class="nav_list">
        <li class="nav_item">
          <a href="/categories/study" id="aa">study
            (15)</a>
        </li>
      </ul>
      
    </nav>
  </div>

</header>



    <nav class="mobile_menu">

  <ul class="nav_list">
    
    <li class="nav_item">
      <a href="/aboutme.html">about</a>
    </li>

    
    
    

    <li class="nav_item">
      <a href="/categories/writing" id="aa">writing (1)</a>
    </li>
    
    

    <li class="nav_item">
      <a href="/categories/study" id="aa">study (15)</a>
    </li>
    
  </ul>

</nav>
    
    <main class="main">

    <div class="post">
  <div>
    
    <p class="post_title">강화학습 2</p>
    
  </div>
  <div class="post_data">
    
    <span class="post_date">Sep 8, 2020</span>
    
    
    <span class="post_categories">
      &raquo; 
      <a href="/categories/study">study</a>
    </span>
    
  </div>
  <div class="post_content">
    <p>Markov Decision Process 문제를 해결한다고 하자.(즉 environment 가 fully observable, state 가 다음 state 를 결정하기에 충분한 정보를 갖는다.) 강화학습을 하기 위해선, Environment 를 Markov Decision Process 로 모델링하는 것이 중요.</p>

<h2 id="max-와-argmax">max 와 argmax</h2>

<p><strong>max 함수</strong>는 어떤 집합 내에서 가장 큰 값을 찾는 함수이다.</p>

<p><strong>argmax 함수</strong>는 인자에 대해서 집합 내에 한 원소가 결정되고, 집합 내에서 가장 큰 원소를 찾았을 때, 그 원소에 대응되는 인자를 반환하는 함수이다. (요컨대 max 가 되게 하는 argument 를 찾는 함수)</p>

<h2 id="state-transition-matrix-의-정의">State Transition Matrix 의 정의</h2>

<p>총 n 개의 state \(s_1, s_2, \cdots , s_n\) 가 있다고 했을 때,</p>

\[P_{ij} = P[ S_{t+1} = s_j \vert S_t = s_i ]\]

<p>이것은 Random 하게 상태가 변경되는 경우에 각 state 에서 다른 state 로 이동할 확률을 명시한 행렬이다.</p>

<h2 id="markov-process">Markov Process</h2>

<p>Markov Process 는 다음과 같이 명시할 수 있다.</p>

\[&lt; S, P &gt;\]

<p>\(S\) : 모든 상태 집합</p>

<p>\(P\) : State Transition Matrix</p>

<p>이 위에서 랜덤하게 State 와 State 를 오간다.</p>

<h2 id="episode">Episode</h2>

<p>Episode 의 정의는 다음과 같다.</p>

<p><strong>어느 state 에서 시작해서 마지막 state 까지 가는 과정</strong></p>

<p>샘플링이라는 것은 확률변수로부터 이벤트가 발생한다는 것을 말하는데, 어떤 확률분포를 갖는 확률변수의 한 사건을 추출한다는 것. 이러한 측면에서 Markov Process 로부터 샘플링을 수행한 것을 episode 라고 볼 수 있겠다.</p>

<h2 id="markov-reward-process">Markov Reward Process</h2>

<p>Markov Reward Process 는 다음과 같이 명시할 수 있다.</p>

\[&lt; S, P, R, \gamma &gt;\]

<p>일반적인 Markov Process 와는 달리 각 상태에 점수가 부여되어 있으며, 해당 상태에 진입할 때마다</p>

<p>\(S\) : 모든 상태 집합</p>

<p>\(P\) : State Transition Matrix</p>

<p>\(R\) : reward function</p>

<p>\(\gamma\) : discount factor</p>

<p>이때 reward function 이란 다음과 같이 정의된다.</p>

\[R_s \equiv E[ Re_{t+1} | S_t = s]\]

<p>어떤 상태에 있을 때, 다음으로 얻을 Reward(\(Re\)) 의 기대값을 말한다.</p>

<h2 id="return">Return</h2>

<p>강화학습은 return \(G_t\) 을 maximize 하는 프로세스이다.</p>

<p>이때 return 은 다음과 같이 정의된다.</p>

\[G_t \equiv Re_{t+1} + \gamma Re_{t + 2} + \gamma^2 Re_{t + 3} + \cdots\]

<p>각 보상 Reward(\(Re\)) 별로 앞에 \(\gamma\) 가 붙어서 미래의 보상에는 좀 더 낮은 가중치를 준다.</p>

<p>discount factor (\(\gamma\)) 를 넣는 이유는 수학적으로 편리하기 때문이라고 한다. 수렴성이 증명된다나.</p>

<p>Markov reward process 가 최종적으로 terminate 될 수 있다면 gamma 가 1 이어도 될 수도 있다.</p>

<h2 id="mrp-의-value-function">MRP 의 Value function</h2>

<p>MRP 에서의 value function 은 return 의 기대값.</p>

<p>episode 를 샘플링하면, 거기에서 return 이 결정되니까 이 return 도 확률변수이다.</p>

\[v(s) = E[ G_t \vert S_t = s ]\]

<h2 id="bellman-equation-for-mrp">Bellman Equation for MRP</h2>

<p>벨만 방정식이라고 부른다.</p>

<p>위의 정의에 따라서 어떤 상태 \(s\) 에서의 value function 과 다음 상태들 \(s'\) 의 value function 사이에 무슨 관계가 있는지를 알려준다.</p>

\[v(s) = E[ G_t \vert S_t = s ]\]

\[= E[ Re_{t+1} + \gamma Re_{t + 2} + \cdots \vert S_t = s ]\]

\[= E[ Re_{t+1} + \gamma G_{t+1} \vert S_t = s ]\]

\[= E[ Re_{t+1} + \gamma v(S_{t + 1}) \vert S_t = s ]\]

\[= R_s + \gamma \sum_{s' \in S}P_{ss'}v(s')\]

<p>이때,</p>

\[V^T = (v(1), v(2), ... , v(n))\]

\[R^T = (R_1, R_2, ... , R_n)\]

<p>이라 하면,</p>

\[V = R + \gamma PV\]

\[(I - \gamma P) V = R\]

\[V = (I - \gamma P)^{-1} R\]

<p>즉, MRP 에서는 value function 이 바로 계산되어진다.</p>

<p>(하지만 계산복잡도가 O^3 이기 때문에, 큰 MRP 문제에서는 Dynamic programming, Monte-Carlo evaluation, Temporal-Difference Learning 을 사용한다.)</p>

<h2 id="markov-decision-process">Markov Decision Process</h2>

<p>Markov Reward Process 는 다음과 같이 명시할 수 있다.</p>

\[&lt; S, A, P, R, \gamma &gt;\]

<p>\(S\) : 모든 상태 집합</p>

<p>\(A\) : action 의 집합</p>

<p>\(P\) : State Transition Matrix</p>

<p>\(R\) : reward function</p>

<p>\(\gamma\) : discount factor</p>

<p>이때 \(P\) 는 MP, MRP 와는 살짝 변형된다.</p>

<p>\(P_{ss'}^a \equiv\) action a 를 했을 때, state 가 s 에서 s’ 로 바뀔 확률이다. 임의의 상태에서 어떤 액션을 했을 때, 이동한 상태가 항상 일정하다면 이를 <strong>Deterministic</strong>, 확률적으로 다른 상태로 이동하기도 한다면 <strong>Stochastic</strong> 하다라고 한다.</p>

<p>또한 \(R\) 도 살짝 변형된다.</p>

<p>\(R_s^a \equiv\) state s 에서 action a 를 수행했을 때 얻는 \(Re_{t + 1}\) 의 기댓값</p>

<p>이번에는 Action 이라는 개념이 추가되어서, Policy 에 따라 다른 action 을 취할 수 있는 가능성을 넣었다. MRP 에서는 랜덤프로세스였지만, MDP 에서는 어떤 액션을 수행할지 선택하는 것이다. 그래서 어떤 policy 를 갖고 액션을 수행할지가 중요하다.</p>

<p>만약 Action 을 하면 확률적으로 다른 state 로 간다.</p>

<p>우선 Agent 가 사용하는 policy 를 수학적으로 다음과 같이 모델링한다.</p>

\[\pi(a\vert s) = P[ A_t = a \vert S_t = s ]\]

<p>MDP 에서 어떤 policy 를 갖고 움직일 때, 만약 policy 가 고정이라면, policy 또한 위의 함수처럼 확률적으로 Action을 결정하기에, MDP 를 MRP 로 볼 수 있다.</p>

<p>이때 MRP 의 명시는 다음과 같이 된다.</p>

\[&lt; S, P^\pi, R^\pi, \gamma &gt;\]

<p>\(P^\pi\) 는 다음과 같이 정의된다.</p>

\[P_{s,s'}^\pi \equiv \sum_{a \in A} \pi(a \vert s)P_{ss'}^a\]

<p>\(R^\pi\) 는 다음과 같이 정의된다.</p>

\[R_s^\pi \equiv \sum_{a \in A} \pi(a \vert s)R_s^a\]

<p>state 의 변화만을 본다면, \(&lt; S, P^\pi &gt;\) 로 명시된 MP 가 된다.</p>

<h2 id="mdp-의-value-function">MDP 의 Value function</h2>

<p>state-value function \(v_\pi(s)\) 는 policy \(\pi\) 를 따라서 끝까지 했을 때, return 의 기대값으로 정의된다.</p>

<p>action-value function \(q_\pi(s, a)\) 는 현재 state s 에서 행동 a 를 했을 때, policy \(\pi\) 를 따랐을 경우의 return 의 기대값. 이것의 좋은 점은 현재 상태가 있을때, 어떤 행동을 하면 좋냐? 라는 질문에 대한 실질적인 답변을 해준다는 점이다.</p>

<h3 id="bellman-expextation-equation-for-mdp">Bellman Expextation Equation for MDP</h3>

\[v_\pi(s) = E_\pi[ Re_{t+1} + \gamma v_\pi(S_{t + 1}) \vert S_t = s ]\]

\[q_\pi(s, a) = E_\pi[ Re_{t+1} + \gamma q_\pi(S_{t + 1}, A_{t + 1}) \vert S_t = s, A_t = a ]\]

<p>v 와 q 사이의 관계는 다음과 같다.</p>

\[v_\pi(s) = \sum_{a \in A}\pi(a \vert s)q_\pi(s, a)\]

\[q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s')\]

<p>따라서</p>

\[v_\pi(s) = \sum_{a \in A}\pi(a \vert s)(R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_\pi(s'))\]

\[q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a ( v_\pi(s') = \sum_{a' \in A}\pi(a' \vert s')q_\pi(s', a') )\]

<p>Bellman Expextation Equation 의 Matrix Form 은 다음과 같다.</p>

\[v_\pi = R^\pi + \gamma P^\pi v_\pi\]

\[v_\pi = (I - \gamma P^\pi)^{-1} R^\pi\]

<h2 id="optimal-value-function">Optimal value function</h2>

<p>optimal state-value function \(v_{*}(s)\) 는 다음을 말한다.</p>

\[v_{*}(s) = \max_\pi v_\pi(s)\]

\[q_{*}(s, a) = \max_\pi q_\pi(s, a)\]

<p>optimal value function 을 알 때, <strong>MDP가 풀렸다.</strong> 라고 한다.</p>

<h2 id="optimal-policy">Optimal policy</h2>

<p>이 경우 policy 와 policy 는 partial ordering 을 갖는다. (집합론으로 치면, ordering 이 가능한 페어와 불가능한 페어가 있다는 것)</p>

\[\pi \geq \pi' \quad if \quad v_\pi(s) \geq v_{\pi'}(s), \forall s\]

<h2 id="mdp-에-대한-theorem">MDP 에 대한 Theorem</h2>

<ul>
  <li>모든 다른 policy 에 대해서 좋거나 같은 optimal policy 가 존재한다.</li>
  <li>
\[v_{\pi*}(s) = v_{*}(s)\]
  </li>
  <li>
\[q_{\pi*}(s, a) = q_{*}(s, a)\]
  </li>
  <li>optimal policy 는 \(q_{*}(s, a)\) 로부터 구해질 수 있다.
\(\pi_{ *}(a \vert s) = 1 \quad if \quad a = \max_{a \in A} q_{ *}(s, a) \quad else \quad 0\)</li>
  <li>어떤 MDP 에도 언제나 deterministic optimal policy 가 있다.</li>
  <li>\(q_{*}(s, a)\) 를 알면, 바로 optimal policy 를 얻는다.</li>
</ul>

<h3 id="bellman-optimality-equation-for-mdp">Bellman Optimality Equation for MDP</h3>

\[v_{*}(s) = \max_{a \in A}q_{ *}(s, a)\]

\[q_{*}(s, a) = R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{ *}(s')\]

\[v_{*}(s) = \max_{a \in A}( R_s^a + \gamma \sum_{s' \in S} P_{ss'}^a v_{ *}(s'))\]

<p>max 로 인해서 linear equation 이 아니기에, 바로 풀 수 없다. (no closed form solution)</p>

<p>이를 풀기 위한 방법론들은 다음과 같다.</p>

<ul>
  <li>Value Iteration</li>
  <li>Policy Iteration</li>
  <li>Q-learning</li>
  <li>Sarsa</li>
</ul>

  </div>


  
  <div class="post_comment">

    

  </div>
  

</div>

    </main>

  </div>

  <footer class="footer">
  <div>
    &copy; 2020 JunWoo Lee.
    Powered by <a href="http://jekyllrb.com" target="_blank">Jekyll</a>.
    Get this theme
    <a href="https://github.com/starry99/catbook" target="_blank">here</a>.
  </div>
</footer>

  <!-- js from https://codepen.io/MrGrigri/pen/XQmWBv -->

<script>
    const themePreference = () => {
        const hasLocalStorage = localStorage.getItem('theme');
        let supports = false;
        let theme = undefined;

        if (hasLocalStorage === 'light') {
            theme = 'light';
        }
        if (hasLocalStorage === 'dark') {
            theme = 'dark';
        }

        if (window.matchMedia(`(prefers_color: dark)`).matches) {
            theme = hasLocalStorage ? hasLocalStorage : 'dark';
            supports = true;
        };
        if (window.matchMedia(`(prefers_color: light)`).matches) {
            theme = hasLocalStorage ? hasLocalStorage : 'light';
            supports = true;
        };
        if (window.matchMedia(`(prefers_color: no-preference)`).matches) {
            theme = hasLocalStorage ? hasLocalStorage : 'none';
            supports = true;
        };

        return {
            supports,
            theme
        };
    }

    document.addEventListener('DOMContentLoaded', e => {
        console.clear();

        const userThemePreference = themePreference();
        const toggle = document.querySelector('[theme_toggle]');
        const html = document.documentElement;

        const setTheme = () => {
            switch (userThemePreference.theme) {
                case 'dark':
                    toggle.checked = true;
                    html.classList.add('dark');
                    html.classList.remove('light');
                    break;
                case 'light':
                    toggle.checked = false;
                    html.classList.remove('dark');
                    html.classList.add('light');
                    break;
            }
        }
        setTheme();
        toggle.addEventListener('click', e => {
            if (toggle.checked) {
                html.classList.add('dark');
                html.classList.remove('light');
                localStorage.setItem('theme', 'dark');
            } else {
                html.classList.remove('dark');
                html.classList.add('light');
                localStorage.setItem('theme', 'light');
            }
        }, false);
    }, false);
</script>
</body>
</html>
