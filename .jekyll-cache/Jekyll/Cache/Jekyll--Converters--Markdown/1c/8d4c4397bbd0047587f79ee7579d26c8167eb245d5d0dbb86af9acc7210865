I"q<h2 id="reinforcement-learning-과-supervised-learning-의-차이">Reinforcement Learning 과 Supervised Learning 의 차이</h2>

<p>Reinforcement Learning (RL) 과 Supervised Learning 의 차이점은 RL 의 경우에는 지도 데이터가 없다는 점이다. Agent 가 스스로 어떤 결정을 내리고, 그것에 대해서 Reward 만을 받는 것으로 학습해 나간다.</p>

<p>만약 목적함수 F 를 정의하고, 이것을 곧 Reward 로 세팅하게되면, 목적함수 F 를 Maximize 하는 액션들을 스스로 학습한다는 점에서, Optimization Problemt Solving 의 한 방법이라고 볼 수 있지 않을까 싶다.</p>

<h2 id="용어-정리">용어 정리</h2>

<h3 id="reward">Reward</h3>

<p>“scalar feedback signal”</p>

<p>t step 에 얻은 reward 를 \(R_t\) 라고 하면,</p>

<p>Agent의 목적은 이렇게 얻어낸 reward 들의 총합을 최대화하는 것을 목적으로 한다.</p>

<h3 id="reward-hypothesis">Reward Hypothesis</h3>

<p>“모든 목적은 cumulative reward 를 극대화하는 것으로 표현될 수 있다.”</p>

<p>요컨대 reward 가 어떤 것을 기준으로 계산되어 Agent 에게 알려지는가가 중요하다.</p>

<h3 id="sequential-decision-making">Sequential Decision Making</h3>

<p>RL은 미래의 Reward 를 최대화하는 행동들을 결정한다. Reward 가 delayed 될 수도 있다. 요컨대 Long-Term reward 를 극대화하는 행동들을 학습한다.</p>

<h3 id="agent">Agent</h3>

<p>뇌와 같은 것(학습자)</p>

<h3 id="environment">Environment</h3>

<p>뇌 외의 모든 것</p>

<h3 id="action">Action</h3>

<p>행동의 단위.</p>

<p>Environment 는 Action을 받고, Reward 와 Observation(변화한 상황, 세상의 현재 모습을 의미?) 을 Agent 에게 알린다.
Agent는 Observation 과 Reward 를 받고 다음 Action 을 결정한다.</p>

<h3 id="history">History</h3>

<p>observation, action, reward 의 나열(기록을 한다.)</p>

\[H_t = O_1, R_1, A_1, ... , A_{t-1}, O_t, R_t\]

<p>Agent 는 History 로부터 Action 을 정한다. Environment 는 History 로부터 Reward 와 Observation 을 정한다.</p>

<h3 id="state">State</h3>

<p>다음에 무슨일이 일어날지 결정할 때 쓰이는 정보. History 로부터 정제된 정보로, 그냥 평범하게 History 에서 무언가 Decision Making 을 하기 편리하게 요리조리 계산한 데이터 더미라고 보면 될 듯 하다.</p>

\[S_t = f(H_t)\]

<p>Envrionment State \(S_t^e\) 는 envirionment 가 다음 observation 과 reward 를 결정하는데 쓰이는 모든 정보들(보통 private)을 의미한다.</p>

<p>Agent State \(S_t^a\) 는 다음 action 을 결정할 때 쓰이는 정보들이다.</p>

<h3 id="어떤-state-가-markov-하다">어떤 State 가 Markov 하다.</h3>

<p>“새로운 state 가 바로 이전 step 의 state 에만 의존하여 결정된다.”</p>

<p>요컨대 과거와는 독립적이고, 현재만이 중요하다.</p>

<p>자동차를 운전한다고 했을 때, 어떤 목적지에 다다르기 위한 의사결정을 한다고 하자. 주변의 모든 사물과 차의 현재 위치, 속도 등을 알면 이전에 내가 어떻게 운전을 해왔는지는 중요하지 않고, 현재의 상태만 있으면 충분하다. 하지만 자동차의 속도 정보를 모른다고 하면, 이후의 state (내가 페달을 더 밟든 말든)는 현재 state 와 이전 history (내가 브레이크를 어느정도 밟아왔고, 페달은 어떻게 밟았는지 등)에 의존하게 된다.</p>

<p>그런 의미에서 infromation state 혹은 Markov state 는 history 로부터 모든 유용한 정보를 전부 담은 state 를 말한다.</p>
:ET