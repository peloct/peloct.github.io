I"r2<p>Q-Table 방법론은 Environment 의 상태와 액션의 수가 적은 경우 사용할 수 있는 방법론이며, Table 이 커짐에 따라, action-value function 을 근사할 수 있는 또 다른 방법론이 필요해진다.</p>

<h2 id="q-network">Q-Network</h2>

<p>Q-Network 는 결국 상태를 입력으로 받으면 액션들의 점수(현재 상황에서 어떤 액션은 몇 점을 기대가능 등)를 반환하는 함수이다. 그리고 그것은 Neural Network 를 통해서 구현된다.</p>

<p>현재 나의 이해로는 세상 어딘가에 optimal 한 action-value function 이 있다고 믿고, Q-Network 는 이 action-value function 을 비교적 적은 인자를 통해서 근사하는 것을 목적으로 하는 방법론이다.</p>

<p>Neural Netwrok 은 결국 Weight 의 모임이고, 각 Weight, 입력에 대한 최종 값의 미분을 구할 수 있기 때문에 Gradient descent method 를 통해서 Optimizing 하는 것이 가능하다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>init Q with random weights

for episode from 1 to M do
    setup environment

    for t from 1 to T do
        p = Q(state)
        action = argmax(p)
        nextState, reward = environment.step(action)
        
        if isTerminal(nextState)
            y = reward
        else
            y = reward + gamma * max(Q(nextState))

        perform gradient descent step on (y - p[action])^2
</code></pre></div></div>

<p>모든 상태에 대해서 각 상태의 Q 가 다음 상태의 최대 Q 와 같을 때, 이 Q 가 optimal 이라는 말일까? 잘 모르겠다. 어쨌든 가장 원하는 상태는 각 Q를 통해서 가장 좋다고 판단되는 선택을 했을 때의 Q 값이, 그 선택으로 인해 이동한 곳에서의 최대 Q 와 같은 경우인듯 하다.</p>

<p>한 번 구현해본 코드.(잘 안된다. 잘못 구현했을 지도 모른다. Keras… 정신나갈거같아정신나갈거같아)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="n">keras</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">as</span> <span class="n">layers</span>


<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">one_hot_table</span><span class="p">[</span><span class="n">x</span><span class="p">:</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">q_prediction</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">q_prediction</span><span class="p">))</span>


<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'FrozenLake-v0'</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="n">render</span><span class="p">()</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">one_hot_table</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># model define
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span>
                       <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                       <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_size</span><span class="p">,)))</span>

<span class="c1">#optimizer = keras.optimizers.SGD(learning_rate=0.05)
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">()</span>

<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="n">rList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rAll</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">fmax</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">q_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">q_p</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_p</span><span class="p">)</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">new_state</span><span class="p">)))</span>
            <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">q_p</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_weights</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_weights</span><span class="p">))</span>

        <span class="n">rAll</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
    <span class="n">rList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">rAll</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Success rate: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rList</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_episodes</span><span class="p">))</span>
</code></pre></div></div>

<p>그런데 위와 같이 국소적인 gradient 를 갖고 step 을 진행할 때마다, 실제로 optimize 한 action-value function 에 수렴하는가 하면 그렇지 않다고 한다. 그 이유는 다음과 같다.</p>

<ul>
  <li>correlated samples : 국소적으로 모인 샘플들은 서로 correlation 을 갖고 있을 가능성이 높고, 그들은 전체 샘플들의 경향성을 잘 대표하지 못한다. (아마 그런 의미일 거야..)</li>
  <li>non-stationary target : gradient descent 를 통해서 target 에 맞도록 q 가 수정되고나면, 그 수정으로 인해 target 도 바뀌기 때문에 target 과의 격차가 다시 생기는 것.</li>
</ul>

<p>위의 문제점들을 수정한 방법론이 <strong>DQN</strong> 이다.</p>

<p>핵심적인 아이디어는 다음과 같다.</p>

<ul>
  <li>
    <p>environment 속에서 action 을 취하면서 바로바로 학습하는 것이 아니라, (state, action, nextState, reward) 를 버퍼에 쌓아둔 이후에, 몇 개씩 샘플링해서 그것을 기반으로 학습시킨다.</p>
  </li>
  <li>
    <p>서로 같은 Q 와 Q’ 의 두 개 준비한다. 학습을 시킬 때 Q’ 는 고정해두고, Q’ 를 통해서 target 을 계산, Q 를 학습시킨다. 이후에 Q’ 를 학습된 Q 로 초기화한다. 이 과정을 반복한다.</p>
  </li>
</ul>
:ET