I"£2<p>Q-Table ë°©ë²•ë¡ ì€ Environment ì˜ ìƒíƒœì™€ ì•¡ì…˜ì˜ ìˆ˜ê°€ ì ì€ ê²½ìš° ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì´ë©°, Table ì´ ì»¤ì§ì— ë”°ë¼, action-value function ì„ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ë¡ ì´ í•„ìš”í•´ì§„ë‹¤.</p>

<h2 id="q-network">Q-Network</h2>

<p>Q-Network ëŠ” ê²°êµ­ ìƒíƒœë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìœ¼ë©´ ì•¡ì…˜ë“¤ì˜ ì ìˆ˜(í˜„ì¬ ìƒí™©ì—ì„œ ì–´ë–¤ ì•¡ì…˜ì€ ëª‡ ì ì„ ê¸°ëŒ€ê°€ëŠ¥ ë“±)ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ê²ƒì€ Neural Network ë¥¼ í†µí•´ì„œ êµ¬í˜„ëœë‹¤.</p>

<p>í˜„ì¬ ë‚˜ì˜ ì´í•´ë¡œëŠ” ì„¸ìƒ ì–´ë”˜ê°€ì— optimal í•œ action-value function ì´ ìˆë‹¤ê³  ë¯¿ê³ , Q-Network ëŠ” ì´ action-value function ì„ ë¹„êµì  ì ì€ ì¸ìë¥¼ í†µí•´ì„œ ê·¼ì‚¬í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤.</p>

<p>Neural Netwrok ì€ ê²°êµ­ Weight ì˜ ëª¨ì„ì´ê³ , ê° Weight, ì…ë ¥ì— ëŒ€í•œ ìµœì¢… ê°’ì˜ ë¯¸ë¶„ì„ êµ¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— Gradient descent method ë¥¼ í†µí•´ì„œ Optimizing í•˜ëŠ” ê²ƒì´ ê°€ëŠ¥í•˜ë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>init Q with random weights

for episode from 1 to M do
    setup environment

    for t from 1 to T do
        p = Q(state)
        action = argmax(p)
        nextState, reward = environment.step(action)
        
        if isTerminal(nextState)
            y = reward
        else
            y = reward + gamma * max(Q(nextState))

        perform gradient descent step on (y - p[action])^2
</code></pre></div></div>

<p>ëª¨ë“  ìƒíƒœì— ëŒ€í•´ì„œ ê° ìƒíƒœì˜ Q ê°€ ë‹¤ìŒ ìƒíƒœì˜ ìµœëŒ€ Q ì™€ ê°™ì„ ë•Œ, ì´ Q ê°€ optimal ì´ë¼ëŠ” ë§ì¼ê¹Œ? ì˜ ëª¨ë¥´ê² ë‹¤. ì–´ì¨Œë“  ê°€ì¥ ì›í•˜ëŠ” ìƒíƒœëŠ” ê° Që¥¼ í†µí•´ì„œ ê°€ì¥ ì¢‹ë‹¤ê³  íŒë‹¨ë˜ëŠ” ì„ íƒì„ í–ˆì„ ë•Œì˜ Q ê°’ì´, ê·¸ ì„ íƒìœ¼ë¡œ ì¸í•´ ì´ë™í•œ ê³³ì—ì„œì˜ ìµœëŒ€ Q ì™€ ê°™ì€ ê²½ìš°ì¸ë“¯ í•˜ë‹¤.</p>

<p>í•œ ë²ˆ êµ¬í˜„í•´ë³¸ ì½”ë“œ.(ì˜ ì•ˆëœë‹¤. ì˜ëª» êµ¬í˜„í–ˆì„ ì§€ë„ ëª¨ë¥¸ë‹¤. Kerasâ€¦ ì •ì‹ ë‚˜ê°ˆê±°ê°™ì•„ì •ì‹ ë‚˜ê°ˆê±°ê°™ì•„)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras</span> <span class="k">as</span> <span class="n">keras</span>
<span class="kn">import</span> <span class="nn">tensorflow.keras.layers</span> <span class="k">as</span> <span class="n">layers</span>


<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">one_hot_table</span><span class="p">[</span><span class="n">x</span><span class="p">:</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">q_prediction</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">square</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">q_prediction</span><span class="p">))</span>


<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'FrozenLake-v0'</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="n">render</span><span class="p">()</span>
<span class="n">input_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">output_size</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span>
<span class="n">one_hot_table</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># model define
</span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="p">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">layers</span><span class="p">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span>
                       <span class="n">kernel_initializer</span><span class="o">=</span><span class="s">'random_uniform'</span><span class="p">,</span>
                       <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">input_size</span><span class="p">,)))</span>

<span class="c1">#optimizer = keras.optimizers.SGD(learning_rate=0.05)
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="n">Adam</span><span class="p">()</span>

<span class="n">num_episodes</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">discount_factor</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="n">rList</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">rAll</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">fmax</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">((</span><span class="n">i</span> <span class="o">//</span> <span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">tf</span><span class="p">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">q_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">state</span><span class="p">))</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">q_p</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">e</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q_p</span><span class="p">)</span>
            <span class="n">new_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">q</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="n">discount_factor</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">one_hot</span><span class="p">(</span><span class="n">new_state</span><span class="p">)))</span>
            <span class="n">loss_value</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">q_p</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="p">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss_value</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_weights</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">trainable_weights</span><span class="p">))</span>

        <span class="n">rAll</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">new_state</span>
    <span class="n">rList</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">rAll</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Success rate: "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">rList</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_episodes</span><span class="p">))</span>
</code></pre></div></div>

<p>ê·¸ëŸ°ë° ìœ„ì™€ ê°™ì´ êµ­ì†Œì ì¸ gradient ë¥¼ ê°–ê³  step ì„ ì§„í–‰í•  ë•Œë§ˆë‹¤, ì‹¤ì œë¡œ optimize í•œ action-value function ì— ìˆ˜ë ´í•˜ëŠ”ê°€ í•˜ë©´ ê·¸ë ‡ì§€ ì•Šë‹¤ê³  í•œë‹¤. ê·¸ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ul>
  <li>correlated samples : êµ­ì†Œì ìœ¼ë¡œ ëª¨ì¸ ìƒ˜í”Œë“¤ì€ ì„œë¡œ correlation ì„ ê°–ê³  ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ê³ , ê·¸ë“¤ì€ ì „ì²´ ìƒ˜í”Œë“¤ì˜ ê²½í–¥ì„±ì„ ì˜ ëŒ€í‘œí•˜ì§€ ëª»í•œë‹¤. (ì•„ë§ˆ ê·¸ëŸ° ì˜ë¯¸ì¼ ê±°ì•¼..)</li>
  <li>non-stationary target : gradient descent ë¥¼ í†µí•´ì„œ target ì— ë§ë„ë¡ q ê°€ ìˆ˜ì •ë˜ê³ ë‚˜ë©´, ê·¸ ìˆ˜ì •ìœ¼ë¡œ ì¸í•´ target ë„ ë°”ë€Œê¸° ë•Œë¬¸ì— target ê³¼ì˜ ê²©ì°¨ê°€ ë‹¤ì‹œ ìƒê¸°ëŠ” ê²ƒ.</li>
</ul>

<p>ìœ„ì˜ ë¬¸ì œì ë“¤ì„ ìˆ˜ì •í•œ ë°©ë²•ë¡ ì´ <strong>DQN</strong> ì´ë‹¤.</p>

<p>í•µì‹¬ì ì¸ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ul>
  <li>
    <p>environment ì†ì—ì„œ action ì„ ì·¨í•˜ë©´ì„œ ë°”ë¡œë°”ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, (state, action, nextState, reward) ë¥¼ ë²„í¼ì— ìŒ“ì•„ë‘” ì´í›„ì—, ëª‡ ê°œì”© ìƒ˜í”Œë§í•´ì„œ ê·¸ê²ƒì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì‹œí‚¨ë‹¤.</p>
  </li>
  <li>
    <p>ì„œë¡œ ê°™ì€ Q ì™€ Qâ€™ ì˜ ë‘ ê°œ ì¤€ë¹„í•œë‹¤. í•™ìŠµì„ ì‹œí‚¬ ë•Œ Qâ€™ ëŠ” ê³ ì •í•´ë‘ê³ , Qâ€™ ë¥¼ í†µí•´ì„œ target ì„ ê³„ì‚°, Q ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤. ì´í›„ì— Qâ€™ ë¥¼ í•™ìŠµëœ Q ë¡œ ì´ˆê¸°í™”í•œë‹¤. ì´ ê³¼ì •ì„ ë°˜ë³µí•œë‹¤.</p>
  </li>
</ul>
:ET