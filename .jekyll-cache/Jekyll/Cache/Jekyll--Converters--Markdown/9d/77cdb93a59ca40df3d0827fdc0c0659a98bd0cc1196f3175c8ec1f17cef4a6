I"<p>Q-Table 방법론은 Environment 의 상태와 액션의 수가 적은 경우 사용할 수 있는 방법론이며, Table 이 커짐에 따라, action-value function 을 근사할 수 있는 또 다른 방법론이 필요해진다.</p>

<h2 id="q-network">Q-Network</h2>

<p>Q-Network 는 결국 상태를 입력으로 받으면 액션들의 점수(현재 상황에서 어떤 액션은 몇 점을 기대가능 등)를 반환하는 함수이다. 그리고 그것은 Neural Network 를 통해서 구현된다.</p>

<p>현재 나의 이해로는 세상 어딘가에 optimal 한 action-value function 이 있다고 믿고, Q-Network 는 이 action-value function 을 비교적 적은 인자를 통해서 근사하는 것을 목적으로 하는 방법론이다.</p>

<p>Neural Netwrok 은 결국 Weight 의 모임이고, 각 Weight, 입력에 대한 최종 값의 미분을 구할 수 있기 때문에 Gradient descent method 를 통해서 Optimizing 하는 것이 가능하다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>init Q with random weights

for episode from 1 to M do
    setup environment

    for t from 1 to T do
        p = Q(state)
        action = argmax(p)
        nextState, reward = environment.step(action)
        
        if isTerminal(nextState)
            y = reward
        else
            y = reward + gamma * max(Q(nextState))

        perform gradient descent step on (y - p[action])^2
</code></pre></div></div>

<p>모든 상태에 대해서 각 상태의 Q 가 다음 상태의 최대 Q + return 과 같을 때, 이 Q 가 optimal 이라는 말일까? 잘 모르겠다. 어쨌든 가장 원하는 상태는 각 state 에 대해서 가장 좋다고 판단되는 선택을 했을 때의 return 의 기대값과 그 선택으로 인해 이동한 곳에서의 최대 return 의 기대값이 같은 경우인 듯 하다.</p>

<p>correlated samples
non-stationary target</p>
:ET