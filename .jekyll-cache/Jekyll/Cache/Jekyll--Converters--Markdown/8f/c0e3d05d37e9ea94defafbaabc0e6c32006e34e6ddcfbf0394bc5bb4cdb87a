I"‚<p>optimal policy ëŠ” ì–´ë–¤ state ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, í•´ë‹¹ state ì—ì„œ optimal action value function Q ì— ëŒ€í•´ argmax í•œ action ì„ deterministic í•˜ê²Œ ìˆ˜í–‰í•˜ëŠ” policy ì´ë‹¤. ì¦‰, optimal policy ë¥¼ ì°¾ëŠ” ê³¼ì •ì€ optimal action value function ì´ë‚˜ optimal state value function ì„ ì°¾ëŠ” ê³¼ì •ì´ë‹¤. optimal value function ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ë“¤ì„ ë§Œì¡±í•œë‹¤. (ì°¸ê³ ë¡œ optimal policy ëŠ” ì—¬ëŸ¬ ê°œ ìˆì„ ìˆ˜ ìˆê³ , partial order ë¥¼ ê°–ëŠ”ë‹¤.)</p>

<p>optimal state value function ê³¼ optimal action value function ì— ëŒ€í•´ì„œ Bellman Equation ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

\[v_*(s) = \max_a q_*(s, a)\]

\[q_*(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a v_*(s')\]

<p>ê·¸ë¦¬ê³  ìœ„ì˜ ì‹ì„ recursive í•˜ê²Œ ì •ë¦¬í•˜ë©´,</p>

\[v_*(s) = \max_a \mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a v_*(s')\]

\[q_*(s, a) = \mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a \max_{a'} q_*(s', a')\]

<p>ì´ë ‡ê²Œ ëœë‹¤. ì—¬ê¸°ì„œ</p>

<p>\(\mathcal{R}_s^a\) : state s ì—ì„œ action a ë¥¼ í–ˆì„ ë•Œ, ì–»ì„ ìˆ˜ ìˆëŠ” Reward ì˜ ê¸°ëŒ“ê°’</p>

\[\mathcal{R}_s^a = E[ R_{t+1} \vert S_t = s, A_t = a ]\]

<p>\(\mathcal{P}_{ss'}^a\) : state s ì—ì„œ action a ë¥¼ í–ˆì„ ë•Œ, state sâ€™ ê°€ ë  í™•ë¥  (gym ì˜ FrozenLake ë¬¸ì œì™€ ê°™ì´ ì–´ë–¤ ì•¡ì…˜ì„ í•œë‹¤ê³  í•´ì„œ, ê·¸ ë‹¤ìŒì˜ ìƒíƒœê°€ í•­ìƒ Deterministic í•˜ê²Œ ê²°ì •ë˜ëŠ”ê²Œ ì•„ë‹ˆë¼, Stochastic í•˜ê²Œ ê²°ì •ë  ìˆ˜ ìˆë‹¤.)</p>

<p>ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ ê³¼ì •ì„ í†µí•˜ë©´ ìœ„ì˜ ìˆ˜ì‹ì„ ë§Œì¡±í•˜ëŠ” value function ë“¤ì„ ê³„ì‚°í•´ë‚´ëŠ”ê°€?</p>

<h2 id="dynamic-programming-ì˜-ê²½ìš°">Dynamic Programming ì˜ ê²½ìš°</h2>

<h3 id="policy-evaluation">Policy Evaluation</h3>

<p>true value function ì„ êµ¬í•˜ëŠ” ê²ƒì„ Policy Evaluation ì´ë¼ í•œë‹¤.</p>

\[v_{k+1}(s) = \sum_{a \in A} \pi(a \vert s)(\mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a v_k(s'))\]

<h3 id="policy-iteration">Policy Iteration</h3>

<p>ê°€ì • : deterministic policy ë¥¼ ì‚¬ìš©í•œë‹¤.</p>

<p>ë‹¤ìŒì„ ë°˜ë³µí•œë‹¤.</p>

<ol>
  <li>í˜„ì¬ policy ì— ëŒ€í•´ì„œ ê° state ì˜ state value function ì„ êµ¬í•œë‹¤. (Policy Evaluation)</li>
  <li>policy ë¥¼ ê° state ì—ì„œ action value function ì„ ê°€ì¥ ìµœëŒ€í™”í•˜ëŠ” action ì„ ì·¨í•˜ëŠ” ê²ƒìœ¼ë¡œ êµì²´í•œë‹¤. (greedy policy)</li>
</ol>

<p>ìœ„ì˜ ê³¼ì •ì„ ë°˜ë³µí•  ë•Œë§ˆë‹¤ ë‹¤ìŒê³¼ ê°™ì´ value function ì´ ê°œì„ ë˜ì–´ê°„ë‹¤.</p>

\[v_{\pi_1}(s) \leq v_{\pi_2}(s) \leq \cdots \leq v_{\pi_n}(s)\]

<p>silver êµìˆ˜ë‹˜ì˜ ê°•ì˜ìë£Œì—ì„œëŠ” policy evaluation ê³¼ policy improvement ë¥¼ ë°˜ë³µí•˜ëŠ” ê²ƒìœ¼ë¡œ ìœ„ì˜ ê³¼ì •ì´ ì„±ë¦½í•¨ì„ ì¦ëª…í•œë‹¤.</p>

<p>ìµœì¢…ì ìœ¼ë¡œ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•ŠëŠ” ë‹¨ê³„ì— ë„ë‹¬í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ë˜ëŠ”ë°,</p>

\[q_\pi(s, \pi'(s)) = \max_{a \in A} q_\pi(s, a) \quad (\because greedy\ improvement)\]

\[= q_\pi(s, \pi(s)) = v_\pi(s)\]

<p>ê·¸ëŸ¬ë¯€ë¡œ</p>

\[v_\pi(s) = \max_{a \in A} q_\pi(s, a)\]

<p>ê·¸ëŸ°ë° ì´ëŠ” í˜„ì¬ value function ì´, optimal value function ì´ ë§Œì¡±ì‹œí‚¤ëŠ” bellman equation ì„ ë§Œì¡±ì‹œí‚´ì„ ë§í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ í˜„ì¬ value function ì€ optimal value function ì´ê³ , ë”°ë¼ì„œ policy ëŠ” optimal policy ê°€ ëœë‹¤.</p>

<h3 id="value-iteration">Value Iteration</h3>

<p>Priniciple of Optimality ë¼ëŠ” Theorem ì´ ìˆë‹¤.</p>

<p>ì´ëŠ” policy ê°€ <strong>state s ì—ì„œ optimal value ë¥¼ ì·¨í•œë‹¤</strong>ë¼ëŠ” ê²ƒì´ <strong>state s ì—ì„œ ê°ˆ ìˆ˜ ìˆëŠ” ëª¨ë“  state sâ€™ ë“¤ì— ëŒ€í•´ optimal value ë¥¼ ì·¨í•œë‹¤</strong>ì™€ ë™ì¹˜ì„ì„ ë§í•œë‹¤.</p>

<p>ê·¸ë˜ì„œ ì´ëŸ¬í•œ ì›ë¦¬ì— ë”°ë¼, ìš°ë¦¬ê°€ ë§Œì•½ ì–´ë–¤ ìƒíƒœ sâ€™ ì—ì„œ optimal value function ì„ êµ¬í–ˆë‹¤ë©´, ê·¸ state ë¡œ action ì„ ì·¨í•´ ê°ˆ ìˆ˜ ìˆëŠ” ì•ì„  state s ì˜ optimal value function ì„ ë‹¤ìŒì˜ ì‹ìœ¼ë¡œ êµ¬í•¨ì— ë”°ë¼ optimal policy ë¥¼ ê°–ëŠ” optimal value function ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. (Policy Iteration ê³¼ ë‹¬ë¦¬ Value function ë§Œì„ ì´ìš©í•œë‹¤ëŠ” ì ì´ ì¤‘ìš”í•˜ë‹¤.)</p>

<p>ë§Œì•½ ìš°ë¦¬ê°€ subproblem \(v_*(s')\) ì˜ í•´ë‹µì„ ì•ˆë‹¤ë©´,</p>

\[v_*(s) \leftarrow \max_{a \in A} \mathcal{R}_s^a + \gamma \sum_{s' \in S} \mathcal{P}_{ss'}^a v_*(s')\]

<p>ìœ„ì˜ ì‹ì„ ì „ì²´ state ì— ëŒ€í•´ì„œ ì—¬ëŸ¬ë²ˆ ë°˜ë³µí•˜ë‹¤ë³´ë©´, optimal value function ì´ êµ¬í•´ì§€ê³ , Principle of Optimality ì— ì˜í•´ optimal policy ë˜í•œ êµ¬í•´ì§„ë‹¤. (ìŒì˜ ê°„ì„  ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆëŠ” graph ì—ì„œì˜ shortest path ë¥¼ ì°¾ëŠ” ë¬¸ì œì™€ ì›ë¦¬ê°€ ë˜‘ê°™ë‹¤.)</p>

:ET